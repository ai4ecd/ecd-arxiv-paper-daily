# arxiv-daily latest papers around wearable device
Automated deployment @ 2023-06-25 20:25:40 Asia/Shanghai
> Welcome to contribute! Add your topics and keywords in [`topic.yml`]({repo_url}/blob/main/database/topic.yml).
> You can also view historical data through the [storage]({repo_url}/blob/main/database/storage).

## tools

### hine
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-22**|**Armed Conflict and Early Human Capital Accumulation: Evidence from Cameroon's Anglophone Conflict**|Hector Galindo-Silva et.al.|[2306.13070v1](http://arxiv.org/abs/2306.13070v1)|null|This paper examines the impact of the Anglophone Conflict in Cameroon on human capital accumulation. Using high-quality individual-level data on test scores and information on conflict-related violent events, a difference-in-differences design is employed to estimate the conflict's causal effects. The results show that an increase in violent events and conflict-related deaths causes a significant decline in test scores in reading and mathematics. The conflict also leads to higher rates of teacher absenteeism and reduced access to electricity in schools. These findings highlight the adverse consequences of conflict-related violence on human capital accumulation, particularly within the Anglophone subsystem. The study emphasizes the disproportionate burden faced by Anglophone pupils due to language-rooted tensions and segregated educational systems.|
|**2023-06-22**|**Quantum Pufferfish Privacy: A Flexible Privacy Framework for Quantum Systems**|Theshani Nuradha et.al.|[2306.13054v1](http://arxiv.org/abs/2306.13054v1)|null|We propose a versatile privacy framework for quantum systems, termed quantum pufferfish privacy (QPP). Inspired by classical pufferfish privacy, our formulation generalizes and addresses limitations of quantum differential privacy by offering flexibility in specifying private information, feasible measurements, and domain knowledge. We show that QPP can be equivalently formulated in terms of the Datta-Leditzky information spectrum divergence, thus providing the first operational interpretation thereof. We reformulate this divergence as a semi-definite program and derive several properties of it, which are then used to prove convexity, composability, and post-processing of QPP mechanisms. Parameters that guarantee QPP of the depolarization mechanism are also derived. We analyze the privacy-utility tradeoff of general QPP mechanisms and, again, study the depolarization mechanism as an explicit instance. The QPP framework is then applied to privacy auditing for identifying privacy violations via a hypothesis testing pipeline that leverages quantum algorithms. Connections to quantum fairness and other quantum divergences are also explored and several variants of QPP are examined.|
|**2023-06-22**|**CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions**|Adian Liusie et.al.|[2306.13047v1](http://arxiv.org/abs/2306.13047v1)|null|Multiple Choice examinations are a ubiquitous form of assessment that is used to measure the ability of candidates across various domains and tasks. Maintaining the quality of proposed questions is of great importance to test designers, and therefore newly proposed questions go through several pre-test evaluation stages before they can be deployed into real-world exams. This process is currently quite manual, which can lead to time lags in the question development cycle. Automating this process would lead to a large improvement in efficiency, however, current datasets do not contain sufficient pre-test analysis information. In this paper, we introduce CamChoice; a multiple-choice comprehension dataset with questions at different target levels, where questions have the true candidate selected options distributions. We introduce the task of candidate distribution matching, propose several evaluation metrics for the task, and demonstrate that automatic systems trained on RACE++ can be leveraged as baselines for our task. We further demonstrate that these automatic systems can be used for practical pre-test evaluation tasks such as detecting underperforming distractors, where our detection systems can automatically identify poor distractors that few candidates select. We release the data publicly for future research.|
|**2023-06-22**|**What to Learn: Features, Image Transformations, or Both?**|Yuxuan Chen et.al.|[2306.13040v1](http://arxiv.org/abs/2306.13040v1)|null|Long-term visual localization is an essential problem in robotics and computer vision, but remains challenging due to the environmental appearance changes caused by lighting and seasons. While many existing works have attempted to solve it by directly learning invariant sparse keypoints and descriptors to match scenes, these approaches still struggle with adverse appearance changes. Recent developments in image transformations such as neural style transfer have emerged as an alternative to address such appearance gaps. In this work, we propose to combine an image transformation network and a feature-learning network to improve long-term localization performance. Given night-to-day image pairs, the image transformation network transforms the night images into day-like conditions prior to feature matching; the feature network learns to detect keypoint locations with their associated descriptor values, which can be passed to a classical pose estimator to compute the relative poses. We conducted various experiments to examine the effectiveness of combining style transfer and feature learning and its training strategy, showing that such a combination greatly improves long-term localization performance.|
|**2023-06-22**|**GT-TSCH: Game-Theoretic Distributed TSCH Scheduler for Low-Power IoT Networks**|Omid Tavallaie et.al.|[2306.13039v1](http://arxiv.org/abs/2306.13039v1)|null|Time-Slotted Channel Hopping (TSCH) is a synchronous medium access mode of the IEEE 802.15.4e standard designed for providing low-latency and highly-reliable end-to-end communication. TSCH constructs a communication schedule by combining frequency channel hopping with Time Division Multiple Access (TDMA). In recent years, IETF designed several standards to define general mechanisms for the implementation of TSCH. However, the problem of updating the TSCH schedule according to the changes of the wireless link quality and node's traffic load left unresolved. In this paper, we use non-cooperative game theory to propose GT-TSCH, a distributed TSCH scheduler designed for low-power IoT applications. By considering selfish behavior of nodes in packet forwarding, GT-TSCH updates the TSCH schedule in a distributed approach with low control overhead by monitoring the queue length, the place of the node in the Directed Acyclic Graph (DAG) topology, the quality of the wireless link, and the data packet generation rate. We prove the existence and uniqueness of Nash equilibrium in our game model and we find the optimal number of TSCH Tx timeslots to update the TSCH slotframe. To examine the performance of our contribution, we implement GT-TSCH on Zolertia Firefly IoT motes and the Contiki-NG Operating System (OS). The evaluation results reveal that GT-TSCH improves performance in terms of throughput and end-to-end delay compared to the state-of-the-art method.|
|**2023-06-22**|**Impacts and Risk of Generative AI Technology on Cyber Defense**|Subash Neupane et.al.|[2306.13033v1](http://arxiv.org/abs/2306.13033v1)|null|Generative Artificial Intelligence (GenAI) has emerged as a powerful technology capable of autonomously producing highly realistic content in various domains, such as text, images, audio, and videos. With its potential for positive applications in creative arts, content generation, virtual assistants, and data synthesis, GenAI has garnered significant attention and adoption. However, the increasing adoption of GenAI raises concerns about its potential misuse for crafting convincing phishing emails, generating disinformation through deepfake videos, and spreading misinformation via authentic-looking social media posts, posing a new set of challenges and risks in the realm of cybersecurity. To combat the threats posed by GenAI, we propose leveraging the Cyber Kill Chain (CKC) to understand the lifecycle of cyberattacks, as a foundational model for cyber defense. This paper aims to provide a comprehensive analysis of the risk areas introduced by the offensive use of GenAI techniques in each phase of the CKC framework. We also analyze the strategies employed by threat actors and examine their utilization throughout different phases of the CKC, highlighting the implications for cyber defense. Additionally, we propose GenAI-enabled defense strategies that are both attack-aware and adaptive. These strategies encompass various techniques such as detection, deception, and adversarial training, among others, aiming to effectively mitigate the risks posed by GenAI-induced cyber threats.|
|**2023-06-22**|**Methodological Reflections on the MOND/Dark Matter Debate**|Patrick M. Duerr et.al.|[2306.13026v1](http://arxiv.org/abs/2306.13026v1)|null|The paper re-examines the principal methodological questions, arising in the debate over the cosmological standard model's postulate of Dark Matter vs. rivalling proposals that modify standard (Newtonian and general-relativistic) gravitational theory, the so-called Modified Newtonian Dynamics (MOND) and its subsequent extensions. What to make of such seemingly radical challenges of cosmological orthodoxy? In the first part of our paper, we assess MONDian theories through the lens of key ideas of major 20th century philosophers of science (Popper, Kuhn, Lakatos, and Laudan), thereby rectifying widespread misconceptions and misapplications of these ideas common in the pertinent MOND-related literature. None of these classical methodological frameworks, which render precise and systematise the more intuitive judgements prevalent in the scientific community, yields a favourable verdict on MOND and its successors -- contrary to claims in the MOND-related literature by some of these theories' advocates; the respective theory appraisals are largely damning. Drawing on these insights, the paper's second part zooms in on the most common complaint about MONDian theories, their ad-hocness. We demonstrate how the recent coherentist model of ad-hocness captures, and fleshes out, the underlying -- but too often insufficiently articulated -- hunches underlying this critique. MONDian theories indeed come out as severely ad hoc: they do not cohere well with either theoretical or empirical-factual background knowledge. In fact, as our complementary comparison with the cosmological standard model's Dark Matter postulate shows, with respect to ad-hocness, MONDian theories fare worse than the cosmological standard model.|
|**2023-06-22**|**Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US**|Jonathan H. Rystrøm et.al.|[2306.13000v1](http://arxiv.org/abs/2306.13000v1)|null|As generative language models are deployed in ever-wider contexts, concerns about their political values have come to the forefront with critique from all parts of the political spectrum that the models are biased and lack neutrality. However, the question of what neutrality is and whether it is desirable remains underexplored. In this paper, I examine neutrality through an audit of Delphi [arXiv:2110.07574], a large language model designed for crowdsourced ethics. I analyse how Delphi responds to politically controversial questions compared to different US political subgroups. I find that Delphi is poorly calibrated with respect to confidence and exhibits a significant political skew. Based on these results, I examine the question of neutrality from a data-feminist lens, in terms of how notions of neutrality shift power and further marginalise unheard voices. These findings can hopefully contribute to a more reflexive debate about the normative questions of alignment and what role we want generative models to play in society.|
|**2023-06-22**|**Can a single image processing algorithm work equally well across all phases of DCE-MRI?**|Adam G. Tattersall et.al.|[2306.12988v1](http://arxiv.org/abs/2306.12988v1)|null|Image segmentation and registration are said to be challenging when applied to dynamic contrast enhanced MRI sequences (DCE-MRI). The contrast agent causes rapid changes in intensity in the region of interest and elsewhere, which can lead to false positive predictions for segmentation tasks and confound the image registration similarity metric. While it is widely assumed that contrast changes increase the difficulty of these tasks, to our knowledge no work has quantified these effects. In this paper we examine the effect of training with different ratios of contrast enhanced (CE) data on two popular tasks: segmentation with nnU-Net and Mask R-CNN and registration using VoxelMorph and VTN. We experimented further by strategically using the available datasets through pretraining and fine tuning with different splits of data. We found that to create a generalisable model, pretraining with CE data and fine tuning with non-CE data gave the best result. This interesting find could be expanded to other deep learning based image processing tasks with DCE-MRI and provide significant improvements to the models performance.|
|**2023-06-22**|**Towards More Realistic Membership Inference Attacks on Large Diffusion Models**|Jan Dubiński et.al.|[2306.12983v1](http://arxiv.org/abs/2306.12983v1)|null|Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known in the cybersecurity community and referred to as a membership inference attack. Our focus is on Stable Diffusion, and we address the challenge of designing a fair evaluation framework to answer this membership question. We propose a methodology to establish a fair evaluation setup and apply it to Stable Diffusion, enabling potential extensions to other generative models. Utilizing this evaluation setup, we execute membership attacks (both known and newly introduced). Our research reveals that previously proposed evaluation setups do not provide a full understanding of the effectiveness of membership inference attacks. We conclude that the membership inference attack remains a significant challenge for large diffusion models (often deployed as black-box systems), indicating that related privacy and copyright issues will persist in the foreseeable future.|
|**2023-06-22**|**On the Degree of Dynamical Packing in the Kepler Multi-planet Systems**|Alysa Obertas et.al.|[2306.12967v1](http://arxiv.org/abs/2306.12967v1)|null|Current planet formation theories rely on initially compact orbital configurations undergoing a (possibly extended) phase of giant impacts following the dispersal of the dissipative protoplanetary disk. The orbital architectures of observed mature exoplanet systems have likely been strongly sculpted by chaotic dynamics, instabilities, and giant impacts. One possible signature of systems continually reshaped by instabilities and mergers is their dynamical packing. Early Kepler data showed that many multi-planet systems are maximally packed - placing an additional planet between an observed pair would make the system unstable. However, this result relied on placing the inserted planet in the most optimistic configuration for stability (e.g., circular orbits). While this would be appropriate in an ordered and dissipative picture of planet formation (i.e. planets dampen into their most stable configurations), we argue that this best-case scenario for stability is rarely realized due to the strongly chaotic nature of planet formation. Consequently, the degree of dynamical packing in multi-planet systems under a realistic formation model is likely significantly higher than previously realized. We examine the full Kepler multi planet sample through this new lens, showing that ~60-95% of Kepler multi-planet systems are strongly packed and that dynamical packing increases with multiplicity. This may be a signature of dynamical sculpting or of undetected planets, showing that dynamical packing is an important metric that can be incorporated into planet formation modelling or when searching for unseen planets.|
|**2023-06-22**|**A New Constraint on the Simulation of the Intergalactic Medium through the Evolution of the Neutral Hydrogen Fraction in the Epoch of Reionization**|S. Mobina Hosseini et.al.|[2306.12954v1](http://arxiv.org/abs/2306.12954v1)|null|The thermal history of the intergalactic medium is full of extremely useful data in the field of astrophysics and cosmology. In other words, by examining this environment in different redshifts, the effects of cosmology and astrophysics can be observed side by side. Therefore, simulation is our very powerful tool to reach a suitable model for the intergalactic medium, both in terms of cosmology and astrophysics. In this work, we have simulated the intergalactic medium with the help of the 21cmFAST code and compared the evolution of the neutral hydrogen fraction in different initial conditions. Considerable works arbitrarily determine many important effective parameters in the thermal history of the intergalactic medium without any constraints, and usually, there is a lot of flexibility for modeling. Nonetheless, in this work, by focusing on the evolution of the neutral hydrogen fraction in different models and comparing it with observational data, we have eliminated many models and introduced only limited simulation models that could confirm the observations with sufficient accuracy. This issue becomes thoroughly vital from the point that, in addition to restricting the models through the neutral hydrogen fraction, it can also impose restrictions on the parameters affecting its changes. However, we hope that in future works, by enhancing the observational data and increasing their accuracy, more compatible models with the history of the intergalactic medium can be achieved.|
|**2023-06-22**|**The source of electrons at comet 67P**|P. Stephenson et.al.|[2306.12942v1](http://arxiv.org/abs/2306.12942v1)|null|We examine the origin of electrons in a weakly outgassing comet, using Rosetta mission data and a 3D collisional model of electrons at a comet. We have calculated a new dataset of electron-impact ionization (EII) frequency throughout the Rosetta escort phase, with measurements of the Rosetta Plasma Consortium's Ion and Electron Sensor (RPC/IES). The EII frequency is evaluated in 15-minute intervals and compared to other Rosetta datasets.   Electron-impact ionization is the dominant source of electrons at 67P away from perihelion and is highly variable (by up to three orders of magnitude). Around perihelion, EII is much less variable and less efficient than photoionization at Rosetta. Several drivers of the EII frequency are identified, including magnetic field strength and the outgassing rate. Energetic electrons are correlated to the Rosetta-upstream solar wind potential difference, confirming that the ionizing electrons are solar wind electrons accelerated by an ambipolar field.   The collisional test particle model incorporates a spherically symmetric, pure water coma and all the relevant electron-neutral collision processes. Electric and magnetic fields are stationary model inputs, and are computed using a fully-kinetic, collisionless Particle-in-Cell simulation. Collisional electrons are modelled at outgassing rates of $Q=10^{26}$ s$^{-1}$ and $Q=1.5\times10^{27}$ s$^{-1}$. Secondary electrons are the dominant population within a weakly outgassing comet. These are produced by collisions of solar wind electrons with the neutral coma.   The implications of large ion flow speed estimates at Rosetta, away from perihelion, are discussed in relation to multi-instrument studies and the new results of the EII frequency obtained in the present study.|
|**2023-06-22**|**Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation**|Ran Zhang et.al.|[2306.12916v1](http://arxiv.org/abs/2306.12916v1)|null|While summarization has been extensively researched in natural language processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a largely unexplored area that has the potential to improve cross-cultural accessibility, information sharing, and understanding. This paper comprehensively addresses the CLCTS task, including dataset creation, modeling, and evaluation. We build the first CLCTS corpus, leveraging historical fictive texts and Wikipedia summaries in English and German, and examine the effectiveness of popular transformer end-to-end models with different intermediate task finetuning tasks. Additionally, we explore the potential of ChatGPT for CLCTS as a summarizer and an evaluator. Overall, we report evaluations from humans, ChatGPT, and several recent automatic evaluation metrics where we find our intermediate task finetuned end-to-end models generate bad to moderate quality summaries; ChatGPT as a summarizer (without any finetuning) provides moderate to good quality outputs and as an evaluator correlates moderately with human evaluations though it is prone to giving lower scores. ChatGPT also seems to be very adept at normalizing historical text. We finally test ChatGPT in a scenario with adversarially attacked and unseen source documents and find that ChatGPT is better at omission and entity swap than negating against its prior knowledge.|
|**2023-06-22**|**Consistent maps and their associated representation theorems**|Charles L. Samuels et.al.|[2306.12887v1](http://arxiv.org/abs/2306.12887v1)|null|A 2009 article of Allcock and Vaaler examined the vector space $\mathcal G := \overline{\mathbb Q}^\times/\overline{\mathbb Q}^\times_{\mathrm{tors}}$ over $\mathbb Q$, describing its completion with respect to the Weil height as a certain $L^1$ space. By involving an object called a consistent map, the author began efforts to establish representation theorems for the duals of spaces related to $\mathcal G$. Specifically, we provided representation theorems for the algebraic and continuous duals of $\overline{\mathbb Q}^\times/{\overline{\mathbb Z}}^\times$. We explore further applications of consistent maps to provide representation theorems for duals of other spaces arising in the work of Allcock and Vaaler. In particular, we establish a connection between consistent maps and locally constant functions on the space of places of $\overline{\mathbb Q}$. We further apply our new results to recover, as a corollary, a main theorem of our previous work.|
|**2023-06-22**|**Estimating dynamic treatment regimes for ordinal outcomes with household interference: Application in household smoking cessation**|Cong Jiang et.al.|[2306.12865v1](http://arxiv.org/abs/2306.12865v1)|null|The focus of precision medicine is on decision support, often in the form of dynamic treatment regimes (DTRs), which are sequences of decision rules. At each decision point, the decision rules determine the next treatment according to the patient's baseline characteristics, the information on treatments and responses accrued by that point, and the patient's current health status, including symptom severity and other measures. However, DTR estimation with ordinal outcomes is rarely studied, and rarer still in the context of interference - where one patient's treatment may affect another's outcome. In this paper, we introduce the proposed weighted proportional odds model (WPOM): a regression-based, doubly-robust approach to single-stage DTR estimation for ordinal outcomes. This method also accounts for the possibility of interference between individuals sharing a household through the use of covariate balancing weights derived from joint propensity scores. Examining different types of balancing weights, we verify the double robustness of WPOM with our adjusted weights via simulation studies. We further extend WPOM to multi-stage DTR estimation with household interference. Lastly, we demonstrate our proposed methodology in the analysis of longitudinal survey data from the Population Assessment of Tobacco and Health study, which motivates this work.|
|**2023-06-22**|**Price elasticity of electricity demand: Using instrumental variable regressions to address endogeneity and autocorrelation of high-frequency time series**|Silvana Tiedemann et.al.|[2306.12863v1](http://arxiv.org/abs/2306.12863v1)|null|This paper examines empirical methods for estimating the response of aggregated electricity demand to high-frequency price signals, the short-term elasticity of electricity demand. We investigate how the endogeneity of prices and the autocorrelation of the time series, which are particularly pronounced at hourly granularity, affect and distort common estimators. After developing a controlled test environment with synthetic data that replicate key statistical properties of electricity demand, we show that not only the ordinary least square (OLS) estimator is inconsistent (due to simultaneity), but so is a regular instrumental variable (IV) regression (due to autocorrelation). Using wind as an instrument, as it is commonly done, may result in an estimate of the demand elasticity that is inflated by an order of magnitude. We visualize the reason for the Thams bias using causal graphs and show that its magnitude depends on the autocorrelation of both the instrument, and the dependent variable. We further incorporate and adapt two extensions of the IV estimation, conditional IV and nuisance IV, which have recently been proposed by Thams et al. (2022). We show that these extensions can identify the true short-term elasticity in a synthetic setting and are thus particularly promising for future empirical research in this field.|
|**2023-06-22**|**Accuracy evaluation of a Low-Cost Differential Global Positioning System for mobile robotics**|Christian Blesing et.al.|[2306.12826v1](http://arxiv.org/abs/2306.12826v1)|null|Differential GPS, commonly referred as DGPS, is a well-known and very accurate localization system for many outdoor applications in particular for mobile outdoor robotics. The most common drawback of DGPS systems are the high costs for both base station and receivers. In this paper, we present a setup that uses third-party open-source software and a Ublox ZED-F9P chip to build a ROS-enabled low-cost DGPS setup that is ready to use in a few hours. The main goal of this paper is to analyze and evaluate the repetitive and absolute accuracy of the system. The first measurement also examines the differences between a SAPOS base station and a locally installed one consisting of low-cost components. During the evaluation process of the absolute accuracy, a moving mobile robot is used on the receiver side. It is tracked through a highly accurate VICON motion capture system.|
|**2023-06-22**|**An Enhanced Massive Black Hole Occupation Fraction Predicted in Cluster Dwarf Galaxies**|Michael Tremmel et.al.|[2306.12813v1](http://arxiv.org/abs/2306.12813v1)|null|The occupation fraction of massive black holes (MBHs) in low mass galaxies offers interesting insights into initial black hole seeding mechanisms and their mass assembly history, though disentangling these two effects remains challenging. Using the Romulus cosmological simulations we examine the impact of environment on the occupation fraction of MBHs in low mass galaxies. Unlike most modern cosmological simulations, Romulus seeds MBHs based on local gas properties, selecting very dense, pristine, and rapidly collapsing regions in the early Universe as sites to host MBHs without assuming anything about MBH occupation as a function of galaxy stellar mass, or halo mass, a priori. The simulations predict that dwarf galaxies with M$_{\star}<10^9$ M$_{\odot}$ in cluster environments are approximately two times more likely to host a MBH compared to those in the field. The predicted occupation fractions are remarkably consistent with those of nuclear star clusters. Across cluster and field environments, dwarf galaxies with earlier formation times are more likely to host a MBH. Thus, while the MBH occupation function is similar between cluster and field environments at high redshift ($z>3$), a difference arises as late-forming dwarfs -- which do not exist in the cluster environment -- begin to dominate in the field and pull the MBH occupation fraction down for low mass galaxies. Additionally, prior to in-fall some cluster dwarfs are similar to progenitors of massive, isolated galaxies, indicating that they might have grown to higher masses had they not been impeded by the cluster environment. While the population of MBHs in dwarf galaxies is already widely understood to be important for understanding MBH formation, this work demonstrates that environmental dependence is important to consider as future observations search for low mass black holes in dwarf galaxies.|
|**2023-06-22**|**Searching for supermassive charged gravitinos in underground experiments**|Krzysztof A. Meissner et.al.|[2306.12797v1](http://arxiv.org/abs/2306.12797v1)|null|We examine possible experimental signatures that may be exploited to search for stable supermassive particles with electric charges of $O(1)$ in future underground experiments, and the upcoming JUNO experiment in particular. The telltale signal would be a correlated sequence of three or more nuclear recoils along a straight line, corresponding to the motion of a non-relativistic ($\beta <10^{-2}$) particle that could enter the detector from any direction. We provide some preliminary estimates for the expected event rates.|
|**2023-06-22**|**Russian assimilatory palatalization is incomplete neutralization**|Sejin Oh et.al.|[2306.12789v1](http://arxiv.org/abs/2306.12789v1)|null|Incomplete neutralization refers to phonetic traces of underlying contrasts in phonologically neutralizing contexts. The present study examines one such context: Russian assimilatory palatalization in C+j sequences. Russian contrasts plain and palatalized consonants, with the plain consonants having a secondary articulation involving retraction of the tongue dorsum (velarization/uvularization). However, Russian also has stop-glide sequences that form near-minimal pairs with palatalized stops. In the environment preceding palatal glides, the contrast between palatalized and plain consonants is neutralized, due to the palatalization of the plain stop (assimilatory palatalization). The purpose of the study is to explore whether the neutralization is complete. To do so, we conducted an electromagnetic articulography (EMA) experiment examining temporal coordination and the spatial position of the tongue body in underlyingly palatalized consonants and those derived from assimilatory palatalization. Articulatory results from four native speakers of Russian revealed that gestures in both conditions are coordinated as complex segments, i.e., they are palatalized consonants. However, there are differences across conditions consistent with the residual presence of a tongue dorsum retraction gesture in the plain obstruents. We conclude that neutralization of the plain-palatal contrast in Russian is incomplete; consonants in the assimilatory palatalization condition exhibit inter-gestural coordination characteristic of palatalized consonants along with residual evidence of an underlying tongue dorsum retraction (velarization/uvularization) gesture.|
|**2023-06-22**|**Arrangement of nearby minima and saddles in the mixed spherical energy landscapes**|Jaron Kent-Dobias et.al.|[2306.12779v1](http://arxiv.org/abs/2306.12779v1)|null|The mixed spherical models were recently found to violate long-held assumptions about mean-field glassy dynamics. In particular, the threshold energy, where most stationary points are marginal and which in the simpler pure models attracts long-time dynamics, seems to lose significance. Here, we compute the typical distribution of stationary points relative to each other in mixed models with a replica symmetric complexity. We examine the stability of nearby points, accounting for the presence of an isolated eigenvalue in their spectrum due to their proximity. Despite finding rich structure not present in the pure models, we find nothing that distinguishes the points that do attract the dynamics. Instead, we find new geometric significance of the old threshold energy, and invalidate pictures of the arrangement most of marginal inherent states into a continuous manifold.|
|**2023-06-22**|**Quark flavour physics: status and future prospects**|Vladimir V. Gligorov et.al.|[2306.12728v1](http://arxiv.org/abs/2306.12728v1)|null|Quark flavour physics is the study of hadrons, their properties, and their decays into other particles. As a discipline it simultaneously catalogues the nature of physical states within the Standard Model of particle physics, and in doing so tests the consistency and completeness of the Standard Model's description of reality. Following the discovery of the Higgs field, it is more essential than ever to critically examine the Standard Model's own coherence. Precision studies of quark flavour are one of the most sensitive experimental instruments for this task. I give a brief and necessarily selective overview of recent developments in quark flavour physics and discuss prospects for the next generation of experiments and facilities, with an emphasis on the energy scales of beyond Standard Model physics probed by these types of measurements.|
|**2023-06-22**|**Multilingual Neural Machine Translation System for Indic to Indic Languages**|Sudhansu Bala Das et.al.|[2306.12693v1](http://arxiv.org/abs/2306.12693v1)|null|This paper gives an Indic-to-Indic (IL-IL) MNMT baseline model for 11 ILs implemented on the Samanantar corpus and analyzed on the Flores-200 corpus. All the models are evaluated using the BLEU score. In addition, the languages are classified under three groups namely East Indo- Aryan (EI), Dravidian (DR), and West Indo-Aryan (WI). The effect of language relatedness on MNMT model efficiency is studied. Owing to the presence of large corpora from English (EN) to ILs, MNMT IL-IL models using EN as a pivot are also built and examined. To achieve this, English- Indic (EN-IL) models are also developed, with and without the usage of related languages. Results reveal that using related languages is beneficial for the WI group only, while it is detrimental for the EI group and shows an inconclusive effect on the DR group, but it is useful for EN-IL models. Thus, related language groups are used to develop pivot MNMT models. Furthermore, the IL corpora are transliterated from the corresponding scripts to a modified ITRANS script, and the best MNMT models from the previous approaches are built on the transliterated corpus. It is observed that the usage of pivot models greatly improves MNMT baselines with AS-TA achieving the minimum BLEU score and PA-HI achieving the maximum score. Among languages, AS, ML, and TA achieve the lowest BLEU score, whereas HI, PA, and GU perform the best. Transliteration also helps the models with few exceptions. The best increment of scores is observed in ML, TA, and BN and the worst average increment is observed in KN, HI, and PA, across all languages. The best model obtained is the PA-HI language pair trained on PAWI transliterated corpus which gives 24.29 BLEU.|
|**2023-06-22**|**PCA matrix denoising is uniform**|Xin T. Tong et.al.|[2306.12690v1](http://arxiv.org/abs/2306.12690v1)|null|Principal component analysis (PCA) is a simple and popular tool for processing high-dimensional data. We investigate its effectiveness for matrix denoising. We assume i.i.d. high dimensional Gaussian noises with standard deviation $\sigma$ are added to clean data generated from a low dimensional subspace. We show that the distance between each pair of PCA-denoised data point and the clean data point is uniformly bounded by $\Otilde(\sigma)$, assuming a low-rank data matrix with mild singular value assumptions. We show such a condition could arise even if the data lies on curves. We then provide a general lower bound for the error of the denoised data matrix, which indicates PCA denoising gives a uniform error bound that is rate-optimal. Furthermore, we examine how the error bound impacts downstream applications such as empirical risk minimization, clustering, and manifold learning. Numerical results validate our theoretical findings and reveal the importance of the uniform error.|
|**2023-06-21**|**Social Media Emotions and IPO Returns**|Domonkos F. Vamossy et.al.|[2306.12602v1](http://arxiv.org/abs/2306.12602v1)|null|I examine potential mechanisms behind two stylized facts of initial public offerings (IPOs) returns. By analyzing investor sentiment expressed on StockTwits and Twitter, I find that emotions conveyed through these social media platforms can help explain the mispricing of IPO stocks. The abundance of information and opinions shared on social media can generate hype around certain stocks, leading to investors' irrational buying and selling decisions. This can result in an overvaluation of the stock in the short term but often leads to a correction in the long term as the stock's performance fails to meet the inflated expectations. In particular, I find that IPOs with high levels of pre-IPO investor enthusiasm tend to have a significantly higher first-day return of 29.54%, compared to IPOs with lower levels of pre-IPO investor enthusiasm, which have an average first-day return of 16.91%. However, this initial enthusiasm may be misplaced, as IPOs with high pre-IPO investor enthusiasm demonstrate a much lower average long-run industry-adjusted return of -8.53%, compared to IPOs with lower pre-IPO investor enthusiasm, which have an average long-run industry-adjusted return of -1.1%.|
|**2023-06-21**|**Investigating the accelerated expansion of the Universe through updated constraints on viable $f(R)$ models within the metric formalism**|Kumar Ravi et.al.|[2306.12585v1](http://arxiv.org/abs/2306.12585v1)|null|Modified theories of gravity encompass a class of $f(R)$-models that seek to elucidate the observed late time accelerated expansion of the universe. In this study, we examine a set of viable $f(R)$ models (Hu-Sawicki: two cases, Satrobinsky, Tsujikawa, exponential and arcTanh models) in metric formalism, using recent cosmological data sets: type Ia supernovae data, cosmic chronometer observations, baryonic acoustic oscillations data, data from H\textsc{ii} starburst galaxies, and local measurements of the Hubble parameter $H_0$. The model parameters are constrained using a Bayesian analysis with the Monte Carlo Markov Chain method. We employ statistical tools such as the Akaike Information Criterion, Bayesian Information Criterion, and reduced chi-square statistics to conduct a comparative investigation of these models. We determine the transition redshift, the evolution of total equation-of-state (EoS) parameter, and the EoS for the component responsible for current accelerated expansion to characterize the expansion's evolution. Taking into account the ``Hubble tension," we perform the study with and without a Gaussian prior for $H_0$ from local measurements. Our findings are as follows: (i) in many cases the $f(R)$ models are strongly favored over the standard $\Lambda$CDM model, (ii) the deviation parameter ($b$) significantly deviates from zero in several cases, (iii) the inclusion of local $H_0$ not only increases the fitted value of $H_0$ (as expected) but also affects the gap between predictions of $f(R)$ models and the $\Lambda$CDM model, and (iv) the relevant quantities characterizing the (accelerated) expansion of the universe obtained in our models are consistent with those obtained in a model-independent way by others. Our investigation and results present a compelling case for pursuing further research on $f(R)$ models with future observations to come.|
|**2023-06-21**|**Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases**|Risako Ando et.al.|[2306.12567v1](http://arxiv.org/abs/2306.12567v1)|null|This paper investigates whether current large language models exhibit biases in logical reasoning, similar to humans. Specifically, we focus on syllogistic reasoning, a well-studied form of inference in the cognitive science of human deduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO, originally designed for psychological experiments that assess human logical abilities in syllogistic reasoning. The dataset consists of syllogistic inferences in both English and Japanese. We examine three types of biases observed in human syllogistic reasoning: belief biases, conversion errors, and atmosphere effects. Our findings demonstrate that current large language models struggle more with problems involving these three types of biases.|
|**2023-06-21**|**Global MHD Simulations of the Time-Dependent Corona**|Roberto Lionello et.al.|[2306.12551v1](http://arxiv.org/abs/2306.12551v1)|null|We describe, test, and apply a technique to incorporate full-sun, surface flux evolution into an MHD model of the global solar corona. Requiring only maps of the evolving surface flux, our method is similar to that of Lionello et al. (2013), but we introduce two ways to correct the electric field at the lower boundary to mitigate spurious currents. We verify the accuracy of our procedures by comparing to a reference simulation, driven with known flows and electric fields. We then present a thermodynamic MHD calculation lasting one solar rotation driven by maps from the magnetic flux evolution model of Schrijver & DeRosa (2003). The dynamic, time-dependent nature of the model corona is illustrated by examining the evolution of the open flux boundaries and forward modeled EUV emission, which evolve in response to surface flows and the emergence and cancellation flux. Although our main goal is to present the method, we briefly investigate the relevance of this evolution to properties of the slow solar wind, examining the mapping of dipped field lines to the topological signatures of the "S-Web" and comparing charge state ratios computed in the time-dependently driven run to a steady state equivalent. Interestingly, we find that driving on its own does not significantly improve the charge states ratios, at least in this modest resolution run that injects minimal helicity. Still, many aspects of the time-dependently driven model cannot be captured with traditional steady-state methods, and such a technique may be particularly relevant for the next generation of solar wind and CME models.|
|**2023-06-21**|**Bayesian inference and role of astrocytes in amyloid-beta dynamics with modelling of Alzheimer's disease using clinical data**|Hina Shaheen et.al.|[2306.12520v1](http://arxiv.org/abs/2306.12520v1)|null|Alzheimer's disease (AD) is a prominent, worldwide, age-related neurodegenerative disease that currently has no systemic treatment. Strong evidence suggests that permeable amyloid-beta peptide (Abeta) oligomers, astrogliosis and reactive astrocytosis cause neuronal damage in AD. A large amount of Abeta is secreted by astrocytes, which contributes to the total Abeta deposition in the brain. This suggests that astrocytes may also play a role in AD, leading to increased attention to their dynamics and associated mechanisms. Therefore, in the present study, we developed and evaluated novel stochastic models for Abeta growth using ADNI data to predict the effect of astrocytes on AD progression in a clinical trial. In the AD case, accurate prediction is required for a successful clinical treatment plan. Given that AD studies are observational in nature and involve routine patient visits, stochastic models provide a suitable framework for modelling AD. Using the approximate Bayesian computation (ABC) approach, the AD etiology may be modelled as a multi-state disease process. As a result, we use this approach to examine the weak and strong influence of astrocytes at multiple disease progression stages using ADNI data from the baseline to 2-year visits for AD patients whose ages ranged from 50 to 90 years. Based on ADNI data, we discovered that the strong astrocyte effect (i.e., a higher concentration of astrocytes as compared to Abeta) could help to lower or clear the growth of Abeta, which is a key to slowing down AD progression.|

## infant

### infant
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-20**|**Deep Learning Methods for Retinal Blood Vessel Segmentation: Evaluation on Images with Retinopathy of Prematurity**|Gorana Gojić et.al.|[2306.11576v1](http://arxiv.org/abs/2306.11576v1)|null|Automatic blood vessel segmentation from retinal images plays an important role in the diagnosis of many systemic and eye diseases, including retinopathy of prematurity. Current state-of-the-art research in blood vessel segmentation from retinal images is based on convolutional neural networks. The solutions proposed so far are trained and tested on images from a few available retinal blood vessel segmentation datasets, which might limit their performance when given an image with retinopathy of prematurity signs. In this paper, we evaluate the performance of three high-performing convolutional neural networks for retinal blood vessel segmentation in the context of blood vessel segmentation on retinopathy of prematurity retinal images. The main motive behind the study is to test if existing public datasets suffice to develop a high-performing predictor that could assist an ophthalmologist in retinopathy of prematurity diagnosis. To do so, we create a dataset consisting solely of retinopathy of prematurity images with retinal blood vessel annotations manually labeled by two observers, where one is the ophthalmologist experienced in retinopathy of prematurity treatment. Experimental results show that all three solutions have difficulties in detecting the retinal blood vessels of infants due to a lower contrast compared to images from public datasets as demonstrated by a significant drop in classification sensitivity. All three solutions segment alongside retinal also choroidal blood vessels which are not used to diagnose retinopathy of prematurity, but instead represent noise and are confused with retinal blood vessels. By visual and numerical observations, we observe that existing solutions for retinal blood vessel segmentation need improvement toward more detailed datasets or deeper models in order to assist the ophthalmologist in retinopathy of prematurity diagnosis.|
|**2023-06-14**|**The generalized hyperbolic family and automatic model selection through the multiple-choice LASSO**|Luca Bagnato et.al.|[2306.08692v1](http://arxiv.org/abs/2306.08692v1)|null|We revisit the generalized hyperbolic (GH) distribution and its nested models. These include widely used parametric choices like the multivariate normal, skew-t, Laplace, and several others. We also introduce the multiple-choice LASSO, a novel penalized method for choosing among alternative constraints on the same parameter. A hierarchical multiple-choice LASSO penalized likelihood is optimized to perform simultaneous model selection and inference within the GH family. We illustrate our approach through a simulation study and a real data example about pre-term infants. The methodology proposed in this paper has been implemented in R functions which are available as supplementary material.|
|**2023-06-13**|**Overfitting Affects the Reliability of Radial Velocity Mass Estimates of the V1298 Tau Planets**|Sarah Blunt et.al.|[2306.08145v1](http://arxiv.org/abs/2306.08145v1)|[link](https://github.com/sblunt/v1298tauri)|Mass, radius, and age measurements of young (<100 Myr) planets have the power to shape our understanding of planet formation. However, young stars tend to be extremely variable in both photometry and radial velocity, which makes constraining these properties challenging. The V1298 Tau system of four ~0.5 Rjup planets transiting a pre-main sequence star presents an important, if stress-inducing, opportunity to directly observe and measure the properties of infant planets. Su\'arez-Mascare\~no et al. (2021) published radial-velocity-derived masses for two of the V1298 Tau planets using a state-of-the-art Gaussian Process regression framework. The planetary densities computed from these masses were surprisingly high, implying extremely rapid contraction after formation in tension with most existing planet formation theories. In an effort to further constrain the masses of the V1298 Tau planets, we obtained 36 RVs using Keck/HIRES, and analyzed them in concert with published RVs and photometry. Through performing a suite of cross validation tests, we found evidence that the preferred model of SM21 suffers from overfitting, defined as the inability to predict unseen data, rendering the masses unreliable. We detail several potential causes of this overfitting, many of which may be important for other RV analyses of other active stars, and recommend that additional time and resources be allocated to understanding and mitigating activity in active young stars such as V1298 Tau.|
|**2023-06-09**|**Two Independent Teachers are Better Role Model**|Afifa Khaled et.al.|[2306.05745v1](http://arxiv.org/abs/2306.05745v1)|[link](https://github.com/AfifaKhaled/Two-Independent-Teachers-are-Better-Role-Model)|Recent deep learning models have attracted substantial attention in infant brain analysis. These models have performed state-of-the-art performance, such as semi-supervised techniques (e.g., Temporal Ensembling, mean teacher). However, these models depend on an encoder-decoder structure with stacked local operators to gather long-range information, and the local operators limit the efficiency and effectiveness. Besides, the $MRI$ data contain different tissue properties ($TPs$) such as $T1$ and $T2$. One major limitation of these models is that they use both data as inputs to the segment process, i.e., the models are trained on the dataset once, and it requires much computational and memory requirements during inference. In this work, we address the above limitations by designing a new deep-learning model, called 3D-DenseUNet, which works as adaptable global aggregation blocks in down-sampling to solve the issue of spatial information loss. The self-attention module connects the down-sampling blocks to up-sampling blocks, and integrates the feature maps in three dimensions of spatial and channel, effectively improving the representation potential and discriminating ability of the model. Additionally, we propose a new method called Two Independent Teachers ($2IT$), that summarizes the model weights instead of label predictions. Each teacher model is trained on different types of brain data, $T1$ and $T2$, respectively. Then, a fuse model is added to improve test accuracy and enable training with fewer parameters and labels compared to the Temporal Ensembling method without modifying the network architecture. Empirical results demonstrate the effectiveness of the proposed method.|
|**2023-06-08**|**Design of Sturm global attractors 2: Time-reversible Chafee-Infante lattices of 3-nose meanders**|Bernold Fiedler et.al.|[2306.05232v1](http://arxiv.org/abs/2306.05232v1)|null|This sequel continues our exploration arxiv:2302.12531 of a deceptively ``simple'' class of global attractors, called Sturm due to nodal properties. They arise for the semilinear scalar parabolic PDE   \begin{equation}\label{eq:*}   u_t = u_{xx} + f(x,u,u_x) \tag{$*$}   \end{equation} on the unit interval $0 < x<1$, under Neumann boundary conditions. This models the interplay of reaction, advection, and diffusion.   Our classification is based on the Sturm meanders, which arise from a shooting approach to the ODE boundary value problem of equilibrium solutions $u=v(x)$. Specifically, we address meanders with only three ``noses'', each of which is innermost to a nested family of upper or lower meander arcs. The Chafee-Infante paradigm of 1974, with cubic nonlinearity $f=f(u)$, features just two noses.   We present, and fully prove, a precise description of global PDE connection graphs, graded by Morse index, for such gradient-like Morse-Smale systems \eqref{eq:*}. The directed edges denote PDE heteroclinic orbits $v_1 \leadsto v_2$ between equilibrium vertices $v_1, v_2$ of adjacent Morse index. The connection graphs can be described as a lattice-like structure of Chafee-Infante subgraphs. However, this simple description requires us to adjoin a single ``equilibrium'' vertex, formally, at Morse level -1. Surprisingly, for parabolic PDEs based on irreversible diffusion, the connection graphs then also exhibit global time reversibility.|
|**2023-06-02**|**BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models**|Marvin Lavechin et.al.|[2306.01506v2](http://arxiv.org/abs/2306.01506v2)|[link](https://github.com/marvinlvn/babyslm)|Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.|
|**2023-06-01**|**Generalizability analyses with a partially nested trial design: the Necrotizing Enterocolitis Surgery Trial**|Sarah E. Robertson et.al.|[2306.00855v1](http://arxiv.org/abs/2306.00855v1)|null|We discuss generalizability analyses under a partially nested trial design, where part of the trial is nested within a cohort of trial-eligible individuals, while the rest of the trial is not nested. This design arises, for example, when only some centers participating in a trial are able to collect data on non-randomized individuals, or when data on non-randomized individuals cannot be collected for the full duration of the trial. Our work is motivated by the Necrotizing Enterocolitis Surgery Trial (NEST) that compared initial laparotomy versus peritoneal drain for infants with necrotizing enterocolitis or spontaneous intestinal perforation. During the first phase of the study, data were collected from randomized individuals as well as consenting non-randomized individuals; during the second phase of the study, however, data were only collected from randomized individuals, resulting in a partially nested trial design. We propose methods for generalizability analyses with partially nested trial designs. We describe identification conditions and propose estimators for causal estimands in the target population of all trial-eligible individuals, both randomized and non-randomized, in the part of the data where the trial is nested, while using trial information spanning both parts. We evaluate the estimators in a simulation study.|
|**2023-05-25**|**Emergence of a phonological bias in ChatGPT**|Juan Manuel Toro et.al.|[2305.15929v2](http://arxiv.org/abs/2305.15929v2)|null|Current large language models, such as OpenAI's ChatGPT, have captured the public's attention because how remarkable they are in the use of language. Here, I demonstrate that ChatGPT displays phonological biases that are a hallmark of human language processing. More concretely, just like humans, ChatGPT has a consonant bias. That is, the chatbot has a tendency to use consonants over vowels to identify words. This is observed across languages that differ in their relative distribution of consonants and vowels such as English and Spanish. Despite the differences in how current artificial intelligence language models are trained to process linguistic stimuli and how human infants acquire language, such training seems to be enough for the emergence of a phonological bias in ChatGPT|
|**2023-05-24**|**From Interactive to Co-Constructive Task Learning**|Anna-Lisa Vollmer et.al.|[2305.15535v1](http://arxiv.org/abs/2305.15535v1)|null|Humans have developed the capability to teach relevant aspects of new or adapted tasks to a social peer with very few task demonstrations by making use of scaffolding strategies that leverage prior knowledge and importantly prior joint experience to yield a joint understanding and a joint execution of the required steps to solve the task. This process has been discovered and analyzed in parent-infant interaction and constitutes a ``co-construction'' as it allows both, the teacher and the learner, to jointly contribute to the task. We propose to focus research in robot interactive learning on this co-construction process to enable robots to learn from non-expert users in everyday situations. In the following, we will review current proposals for interactive task learning and discuss their main contributions with respect to the entailing interaction. We then discuss our notion of co-construction and summarize research insights from adult-child and human-robot interactions to elucidate its nature in more detail. From this overview we finally derive research desiderata that entail the dimensions architecture, representation, interaction and explainability.|
|**2023-05-22**|**Developmental Curiosity and Social Interaction in Virtual Agents**|Chris Doyle et.al.|[2305.13396v1](http://arxiv.org/abs/2305.13396v1)|null|Infants explore their complex physical and social environment in an organized way. To gain insight into what intrinsic motivations may help structure this exploration, we create a virtual infant agent and place it in a developmentally-inspired 3D environment with no external rewards. The environment has a virtual caregiver agent with the capability to interact contingently with the infant agent in ways that resemble play. We test intrinsic reward functions that are similar to motivations that have been proposed to drive exploration in humans: surprise, uncertainty, novelty, and learning progress. These generic reward functions lead the infant agent to explore its environment and discover the contingencies that are embedded into the caregiver agent. The reward functions that are proxies for novelty and uncertainty are the most successful in generating diverse experiences and activating the environment contingencies. We also find that learning a world model in the presence of an attentive caregiver helps the infant agent learn how to predict scenarios with challenging social and physical dynamics. Taken together, our findings provide insight into how curiosity-like intrinsic rewards and contingent social interaction lead to dynamic social behavior and the creation of a robust predictive world model.|
|**2023-05-21**|**Towards Robust Family-Infant Audio Analysis Based on Unsupervised Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio**|Jialu Li et.al.|[2305.12530v2](http://arxiv.org/abs/2305.12530v2)|null|To perform automatic family audio analysis, past studies have collected recordings using phone, video, or audio-only recording devices like LENA, investigated supervised learning methods, and used or fine-tuned general-purpose embeddings learned from large pretrained models. In this study, we advance the audio component of a new infant wearable multi-modal device called LittleBeats (LB) by learning family audio representation via wav2vec 2.0 (W2V2) pertaining. We show given a limited number of labeled LB home recordings, W2V2 pretrained using 1k-hour of unlabeled home recordings outperforms oracle W2V2 pretrained on 52k-hour unlabeled audio in terms of parent/infant speaker diarization (SD) and vocalization classifications (VC) at home. Extra relevant external unlabeled and labeled data further benefit W2V2 pretraining and fine-tuning. With SpecAug and environmental speech corruptions, we obtain 12% relative gain on SD and moderate boost on VC. Code and model weights are available.|
|**2023-05-18**|**Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses**|Eliza Kosoy et.al.|[2305.11243v1](http://arxiv.org/abs/2305.11243v1)|null|Developmental psychologists have spent decades devising experiments to test the intelligence and knowledge of infants and children, tracing the origin of crucial concepts and capacities. Moreover, experimental techniques in developmental psychology have been carefully designed to discriminate the cognitive capacities that underlie particular behaviors. We propose that using classical experiments from child development is a particularly effective way to probe the computational abilities of AI models, in general, and LLMs in particular. First, the methodological techniques of developmental psychology, such as the use of novel stimuli to control for past experience or control conditions to determine whether children are using simple associations, can be equally helpful for assessing the capacities of LLMs. In parallel, testing LLMs in this way can tell us whether the information that is encoded in text is sufficient to enable particular responses, or whether those responses depend on other kinds of information, such as information from exploration of the physical world. In this work we adapt classical developmental experiments to evaluate the capabilities of LaMDA, a large language model from Google. We propose a novel LLM Response Score (LRS) metric which can be used to evaluate other language models, such as GPT. We find that LaMDA generates appropriate responses that are similar to those of children in experiments involving social understanding, perhaps providing evidence that knowledge of these domains is discovered through language. On the other hand, LaMDA's responses in early object and action understanding, theory of mind, and especially causal reasoning tasks are very different from those of young children, perhaps showing that these domains require more real-world, self-initiated exploration and cannot simply be learned from patterns in language input.|
|**2023-05-18**|**Client Selection for Federated Policy Optimization with Environment Heterogeneity**|Zhijie Xie et.al.|[2305.10978v3](http://arxiv.org/abs/2305.10978v3)|null|The development of Policy Iteration (PI) has inspired many recent algorithms for Reinforcement Learning (RL), including several policy gradient methods, that gained both theoretical soundness and empirical success on a variety of tasks. The theory of PI is rich in the context of centralized learning, but its study is still in the infant stage under the federated setting. This paper explores the federated version of Approximate PI (API) and derives its error bound, taking into account the approximation error introduced by environment heterogeneity. We theoretically prove that a proper client selection scheme can reduce this error bound. Based on the theoretical result, we propose a client selection algorithm to alleviate the additional approximation error caused by environment heterogeneity. Experiment results show that the proposed algorithm outperforms other biased and unbiased client selection methods on the federated mountain car problem by effectively selecting clients with a lower level of heterogeneity from the population distribution.|
|**2023-05-17**|**Analyzing the Stance of Facebook Posts on Abortion Considering State-level Health and Social Compositions**|Ana Aleksandric et.al.|[2305.09889v1](http://arxiv.org/abs/2305.09889v1)|null|Abortion remains one of the most controversial topics, especially after overturning Roe v. Wade ruling in the United States. Previous literature showed that the illegality of abortion could have serious consequences, as women might seek unsafe pregnancy terminations leading to increased maternal mortality rates and negative effects on their reproductive health. Therefore, the stances of the abortion-related Facebook posts were analyzed at the state level in the United States from May 4 until June 30, 2022, right after the Supreme Court's decision was disclosed. In more detail, the pre-trained Transformer architecture-based model was fine-tuned on a manually labeled training set to obtain a stance detection model suitable for the collected dataset. Afterward, we employed appropriate statistical tests to examine the relationships between public opinion regarding abortion, abortion legality, political leaning, and factors measuring the overall population's health, health knowledge, and vulnerability per state. We found that states with a higher number of views against abortion also have higher infant and maternal mortality rates. Furthermore, the stance of social media posts per state is mostly matching with the current abortion laws in these states. While aligned with existing literature, these findings indicate how public opinion, laws, and women's and infants' health are related, and interventions are required to educate and protect women, especially in vulnerable populations.|
|**2023-05-16**|**Evaluation of self-supervised pre-training for automatic infant movement classification using wearable movement sensors**|Einari Vaaras et.al.|[2305.09366v1](http://arxiv.org/abs/2305.09366v1)|[link](https://github.com/SPEECHCOG/data2vec_maiju)|The recently-developed infant wearable MAIJU provides a means to automatically evaluate infants' motor performance in an objective and scalable manner in out-of-hospital settings. This information could be used for developmental research and to support clinical decision-making, such as detection of developmental problems and guiding of their therapeutic interventions. MAIJU-based analyses rely fully on the classification of infant's posture and movement; it is hence essential to study ways to increase the accuracy of such classifications, aiming to increase the reliability and robustness of the automated analysis. Here, we investigated how self-supervised pre-training improves performance of the classifiers used for analyzing MAIJU recordings, and we studied whether performance of the classifier models is affected by context-selective quality-screening of pre-training data to exclude periods of little infant movement or with missing sensors. Our experiments show that i) pre-training the classifier with unlabeled data leads to a robust accuracy increase of subsequent classification models, and ii) selecting context-relevant pre-training data leads to substantial further improvements in the classifier performance.|
|**2023-05-16**|**Health Impacts of Public Pawnshops in Industrializing Tokyo**|Tatsuki Inoue et.al.|[2305.09352v1](http://arxiv.org/abs/2305.09352v1)|null|This study is the first to investigate whether financial institutions for low-income populations have contributed to the historical decline in mortality rates. Using ward-level panel data from prewar Tokyo City, we found that public pawn loans were associated with reductions in infant and fetal death rates, potentially through improved nutrition and hygiene measures. Simple calculations suggest that popularizing public pawnshops led to a 6% and 8% decrease in infant mortality and fetal death rates, respectively, from 1927 to 1935. Contrarily, private pawnshops showed no significant association with health improvements. Our findings enrich the expanding literature on demographics and financial histories.|
|**2023-05-09**|**Child Palm-ID: Contactless Palmprint Recognition for Children**|Akash Godbole et.al.|[2305.05161v1](http://arxiv.org/abs/2305.05161v1)|null|Effective distribution of nutritional and healthcare aid for children, particularly infants and toddlers, in some of the least developed and most impoverished countries of the world, is a major problem due to the lack of reliable identification documents. Biometric authentication technology has been investigated to address child recognition in the absence of reliable ID documents. We present a mobile-based contactless palmprint recognition system, called Child Palm-ID, which meets the requirements of usability, hygiene, cost, and accuracy for child recognition. Using a contactless child palmprint database, Child-PalmDB1, consisting of 19,158 images from 1,020 unique palms (in the age range of 6 mos. to 48 mos.), we report a TAR=94.11% @ FAR=0.1%. The proposed Child Palm-ID system is also able to recognize adults, achieving a TAR=99.4% on the CASIA contactless palmprint database and a TAR=100% on the COEP contactless adult palmprint database, both @ FAR=0.1%. These accuracies are competitive with the SOTA provided by COTS systems. Despite these high accuracies, we show that the TAR for time-separated child-palmprints is only 78.1% @ FAR=0.1%.|
|**2023-05-03**|**Analysing the Impact of Audio Quality on the Use of Naturalistic Long-Form Recordings for Infant-Directed Speech Research**|María Andrea Cruz Blandón et.al.|[2305.01965v1](http://arxiv.org/abs/2305.01965v1)|[link](https://github.com/SPEECHCOG/ids_audio_quality_analysis)|Modelling of early language acquisition aims to understand how infants bootstrap their language skills. The modelling encompasses properties of the input data used for training the models, the cognitive hypotheses and their algorithmic implementations being tested, and the evaluation methodologies to compare models to human data. Recent developments have enabled the use of more naturalistic training data for computational models. This also motivates development of more naturalistic tests of model behaviour. A crucial step towards such an aim is to develop representative speech datasets consisting of speech heard by infants in their natural environments. However, a major drawback of such recordings is that they are typically noisy, and it is currently unclear how the sound quality could affect analyses and modelling experiments conducted on such data. In this paper, we explore this aspect for the case of infant-directed speech (IDS) and adult-directed speech (ADS) analysis. First, we manually and automatically annotated audio quality of utterances extracted from two corpora of child-centred long-form recordings (in English and French). We then compared acoustic features of IDS and ADS in an in-lab dataset and across different audio quality subsets of naturalistic data. Finally, we assessed how the audio quality and recording environment may change the conclusions of a modelling analysis using a recent self-supervised learning model. Our results show that the use of modest and high audio quality naturalistic speech data result in largely similar conclusions on IDS and ADS in terms of acoustic analyses and modelling experiments. We also found that an automatic sound quality assessment tool can be used to screen out useful parts of long-form recordings for a closer analysis with comparable results to that of manual quality annotation.|
|**2023-05-02**|**DeCom: Deep Coupled-Factorization Machine for Post COVID-19 Respiratory Syncytial Virus Prediction with Nonpharmaceutical Interventions Awareness**|Xinyan Li et.al.|[2305.01770v1](http://arxiv.org/abs/2305.01770v1)|null|Respiratory syncytial virus (RSV) is one of the most dangerous respiratory diseases for infants and young children. Due to the nonpharmaceutical intervention (NPI) imposed in the COVID-19 outbreak, the seasonal transmission pattern of RSV has been discontinued in 2020 and then shifted months ahead in 2021 in the northern hemisphere. It is critical to understand how COVID-19 impacts RSV and build predictive algorithms to forecast the timing and intensity of RSV reemergence in post-COVID-19 seasons. In this paper, we propose a deep coupled tensor factorization machine, dubbed as DeCom, for post COVID-19 RSV prediction. DeCom leverages tensor factorization and residual modeling. It enables us to learn the disrupted RSV transmission reliably under COVID-19 by taking both the regular seasonal RSV transmission pattern and the NPI into consideration. Experimental results on a real RSV dataset show that DeCom is more accurate than the state-of-the-art RSV prediction algorithms and achieves up to 46% lower root mean square error and 49% lower mean absolute error for country-level prediction compared to the baselines.|
|**2023-05-02**|**Self-supervised learning for infant cry analysis**|Arsenii Gorin et.al.|[2305.01578v1](http://arxiv.org/abs/2305.01578v1)|null|In this paper, we explore self-supervised learning (SSL) for analyzing a first-of-its-kind database of cry recordings containing clinical indications of more than a thousand newborns. Specifically, we target cry-based detection of neurological injury as well as identification of cry triggers such as pain, hunger, and discomfort. Annotating a large database in the medical setting is expensive and time-consuming, typically requiring the collaboration of several experts over years. Leveraging large amounts of unlabeled audio data to learn useful representations can lower the cost of building robust models and, ultimately, clinical solutions. In this work, we experiment with self-supervised pre-training of a convolutional neural network on large audio datasets. We show that pre-training with SSL contrastive loss (SimCLR) performs significantly better than supervised pre-training for both neuro injury and cry triggers. In addition, we demonstrate further performance gains through SSL-based domain adaptation using unlabeled infant cries. We also show that using such SSL-based pre-training for adaptation to cry sounds decreases the need for labeled data of the overall system.|
|**2023-05-01**|**CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds**|David Budaghyan et.al.|[2305.00969v3](http://arxiv.org/abs/2305.00969v3)|[link](https://github.com/ubenwa/cryceleb2023)|This paper describes the Ubenwa CryCeleb dataset - a labeled collection of infant cries, and the accompanying CryCeleb 2023 task - a public speaker verification challenge based on infant cry sounds. We release for academic usage more than 6 hours of manually segmented cry sounds from 786 newborns to encourage research in infant cry analysis.|
|**2023-04-27**|**Cluster Flow: how a hierarchical clustering layer make allows deep-NNs more resilient to hacking, more human-like and easily implements relational reasoning**|Ella Gale et.al.|[2304.14081v1](http://arxiv.org/abs/2304.14081v1)|null|Despite the huge recent breakthroughs in neural networks (NNs) for artificial intelligence (specifically deep convolutional networks) such NNs do not achieve human-level performance: they can be hacked by images that would fool no human and lack `common sense'. It has been argued that a basis of human-level intelligence is mankind's ability to perform relational reasoning: the comparison of different objects, measuring similarity, grasping of relations between objects and the converse, figuring out the odd one out in a set of objects. Mankind can even do this with objects they have never seen before. Here we show how ClusterFlow, a semi-supervised hierarchical clustering framework can operate on trained NNs utilising the rich multi-dimensional class and feature data found at the pre-SoftMax layer to build a hyperspacial map of classes/features and this adds more human-like functionality to modern deep convolutional neural networks. We demonstrate this with 3 tasks. 1. the statistical learning based `mistakes' made by infants when attending to images of cats and dogs. 2. improving both the resilience to hacking images and the accurate measure of certainty in deep-NNs. 3. Relational reasoning over sets of images, including those not known to the NN nor seen before. We also demonstrate that ClusterFlow can work on non-NN data and deal with missing data by testing it on a Chemistry dataset. This work suggests that modern deep NNs can be made more human-like without re-training of the NNs. As it is known that some methods used in deep and convolutional NNs are not biologically plausible or perhaps even the best approach: the ClusterFlow framework can sit on top of any NN and will be a useful tool to add as NNs are improved in this regard.|
|**2023-04-19**|**Detection of a Super-Virial Hot Component in the Milky Way Circumgalactic Medium Along Multiple Sight-Lines by Using the Stacking Technique**|Armando Lara-DI et.al.|[2304.09641v1](http://arxiv.org/abs/2304.09641v1)|null|The study of the elusive hot component ($T \gtrsim 10^7$ K) of the Milky Way circumgalactic medium (CGM) is a novel topic to understand Galactic formation and evolution. In this work, we use the stacking technique through 46 lines of sight with Chandra ACIS-S HETG totaling over 10Ms of exposure time and 9 lines of sight with ACIS-S LETG observations totaling over 1Ms of exposure time, to study in absorption the presence of highly ionized metals arising from the super-virial temperature phase of the CGM. Focusing in the spectral range $4 - 8$ $\r{A}$, we were able to confirm the presence of this hot phase with high significance. We detected transitions of Si XIV K$\alpha$ (with total significance of 6.0$\sigma$) and, for the first time, SXVI K (total significance 4.8$\sigma$) in the rest frame of our own Galaxy. For S XVI K$\alpha$ we found a column density of $1.50^{+0.44}_{-0.38} \times 10^{16} \mathrm{cm}^{-2}$. For Si XIV K$\alpha$ we measured a column density of $0.87\pm{0.16} \times 10^{16} \mathrm{cm}^{-2}$. The lines of sight used in this work are spread across the sky, probing widely separated regions of the CGM. Therefore, our results indicate that this newly discovered hot medium extends throughout the halo, and is not related only to the Galactic Bubbles. The hot gas location, distribution, and covering factor, however, remain unknown. This component might contribute significantly to the missing baryons and metals in the Milky Way.|
|**2023-04-08**|**Predator-prey dynamics pertaining to structuralizing predator species into three stages coupled with maturation delay owing to juvenile hunting**|Debasish Bhattacharjee et.al.|[2304.05159v1](http://arxiv.org/abs/2304.05159v1)|null|The predator-prey dynamic appertaining to two species is explored, wherein the predator species is structured into different stages. As evidenced from natural documentation, the immature predators possess the potential to predate albeit not as competently as the adults. Nevertheless, this potentiality is not acquired immediately after their incipience of life, hence, the immature stage is branched off into the infant stage, the stage with extensive reliance on the adults, and the juvenile stage, the stage with the potential to predate but not to procreate. In this paper, this inaugural concept is coupled with injuries in the juvenile stage as the repercussion of their incompetency in predating, thereby ensuing a delay in their maturation. With the incentive to investigate the ascendancy of these refinements over the whole system, stability analyses along with various bifurcation analyses around the equilibrium points of the system are corroborated. In addition to Hopf, transcritical, and saddle node bifurcations, the existence of Bogdanov-Takens point, cusp point, Bautin bifurcation point, bloom phenomenon, twice occurring Hopf bifurcation, and bi-stability phenomenon make the paper appreciably more rich and efficacious.|
|**2023-04-05**|**IHCV: Discovery of Hidden Time-Dependent Control Variables in Non-Linear Dynamical Systems**|Juan Munoz et.al.|[2304.02443v1](http://arxiv.org/abs/2304.02443v1)|null|Discovering non-linear dynamical models from data is at the core of science. Recent progress hinges upon sparse regression of observables using extensive libraries of candidate functions. However, it remains challenging to model hidden non-observable control variables governing switching between different dynamical regimes. Here we develop a data-efficient derivative-free method, IHCV, for the Identification of Hidden Control Variables. First, the performance and robustness of IHCV against noise are evaluated by benchmarking the IHCV method using well-known bifurcation models (saddle-node, transcritical, pitchfork, Hopf). Next, we demonstrate that IHCV discovers hidden driver variables in the Lorenz, van der Pol, Hodgkin-Huxley, and Fitzhugh-Nagumo models. Finally, IHCV generalizes to the case when only partial observational is given, as demonstrated using the toggle switch model, the genetic repressilator oscillator, and a Waddington landscape model. Our proof-of-principle illustrates that utilizing normal forms could facilitate the data-efficient and scalable discovery of hidden variables controlling transitions between different dynamical regimes and non-linear models.|
|**2023-04-02**|**Origin of high-velocity ejecta and early red excess emission in the infant Type Ia supernova 2021aefx**|Yuan Qi Ni et.al.|[2304.00625v2](http://arxiv.org/abs/2304.00625v2)|null|\object{SN 2021aefx} is a normal Type Ia Supernova (SN) with red excess emission over the first $\sim$ 2 days. We present detailed analysis of this SN using our high-cadence KMTNet multi-band photometry, spectroscopy, and publicly available data. We provide the first measurements of its epochs of explosion (MJD 59529.32 $\pm$ 0.16) as well as ``first light'' (MJD 59529.85 $\pm$ 0.55) associated with the main ejecta ${\rm{^{56}Ni}}$ distribution. This places our first detection of SN 2021aefx at $\sim -$0.5 hours since ``first light'', indicating the presence of additional power sources. Our peak-spectrum confirms its Type Ia sub-classification as intermediate between Core-Normal and Broad-Line, and we estimate the ejecta mass to be $\sim$ 1.34 $M_{\odot}$. The pre-peak spectral evolution identifies fast-expanding material reaching $>$ 40,000 km s$^{-1}$ (the fastest ever observed in Type Ia SNe) and at least two distinct homologously-expanding ejecta components: (1) a normal-velocity (12,400 km s$^{-1}$) component consistent with the typical photospheric evolution of Chandrasekhar-mass ejecta; and (2) a high-velocity (23,500 km s$^{-1}$) component visible during the first $\sim$ 3.6 days post-explosion, which locates the component within the outer $<$ 16\% of the ejecta mass. Asymmetric, subsonic explosion processes producing a non-spherical photosphere provide an explanation for the simultaneous presence of the two components, as well as the red excess emission via a slight ${\rm{^{56}Ni}}$ enrichment in the outer $\sim$ 0.5\% of the ejecta mass. Our spectrum from 300 days post-peak advances the constraint against non-degenerate companions and further supports a near-Chandrasekhar-mass explosion origin. Off-center ignited delayed-detonations of Chandrasekhar-mass white dwarfs may be responsible for the observed features of SN 2021aefx in some normal Type Ia SNe.|
|**2023-04-02**|**The short- and long-term determinants of fertility in Uruguay**|Zuleika Ferre et.al.|[2304.00539v1](http://arxiv.org/abs/2304.00539v1)|null|This paper examines the determinants of fertility among women at different stages of their reproductive lives in Uruguay. To this end, we employ time series analysis methods based on data from 1968 to 2021 and panel data techniques based on department-level statistical information from 1984 to 2019. The results of our first econometric exercise indicate a cointegration relationship between fertility and economic performance, education and infant mortality, with differences observed by reproductive stage. We find a negative relationship between income and fertility for women aged 20-29 that persists for women aged 30 and over. This result suggests that having children is perceived as an opportunity cost for women in this age group. We also observe a negative relationship between education and adolescent fertility, which has implications for the design of public policies. A panel data analysis with econometric techniques allowing us to control for unobserved heterogeneity confirms that income is a relevant factor for all groups of women and reinforces the crucial role of education in reducing teenage fertility. We also identify a negative correlation between fertility and employment rates for women aged 30 and above. We outline some possible explanations for these findings in the context of work-life balance issues and argue for the importance of implementing social policies to address them.|
|**2023-04-01**|**N,N,N-Trimethyl chitosan as a permeation enhancer for inhalation drug delivery: interaction with a model pulmonary surfactant**|Jana Szabová et.al.|[2304.04547v1](http://arxiv.org/abs/2304.04547v1)|null|N,N,N-Trimethyl chitosan (TMC), a biocompatible and biodegradable derivative of chitosan, is currently used as a permeation enhancer to increase the translocation of drugs to the bloodstream in the lungs. This article discusses the effect of TMC on a mimetic pulmonary surfactant, Curosurf, a low-viscosity lipid formulation administered to preterm infants with acute respiratory distress syndrome. Curosurf exhibits a strong interaction with TMC, resulting in the formation of aggregates at electrostatic charge stoichiometry. At nanoscale, Curosurf undergoes a profound reorganization of its lipid vesicles in terms of size and lamellarity. The initial micron-sized vesicles (average size 4.8 microns) give way to a froth-like network of unilamellar vesicles about 300 nm in size. Under such conditions, neutralization of the cationic charges by pulmonary surfactant may inhibit TMC permeation enhancer capacity, especially as electrostatic charge complexation is found at low TMC content. The permeation properties of pulmonary surfactant-neutralized TMC should then be evaluated for its applicability as a permeation enhancer for inhalation in the alveolar region.|
|**2023-03-31**|**Symmetry Groupoids for Pattern-Selective Feedback Stabilization of the Chafee--Infante Equation**|Isabelle Schneider et.al.|[2303.18078v2](http://arxiv.org/abs/2303.18078v2)|null|Reaction-diffusion equations are ubiquitous in various scientific domains and their patterns represent a fascinating area of investigation. However, many of these patterns are unstable and therefore challenging to observe. To overcome this limitation, we present new noninvasive feedback controls based on symmetry groupoids. As a concrete example, we employ these controls to selectively stabilize unstable equilibria of the Chafee--Infante equation under Dirichlet boundary conditions on the interval. Unlike conventional reflection-based control schemes, our approach incorporates additional symmetries that enable us to design new convolution controls for stabilization. By demonstrating the efficacy of our method, we provide a new tool for investigating and controlling systems with unstable patterns, with potential implications for a wide range of scientific disciplines.|
|**2023-03-29**|**A Video-based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants**|Shaotong Zhu et.al.|[2303.16867v1](http://arxiv.org/abs/2303.16867v1)|[link](https://github.com/ostadabbas/nns-detection-and-segmentation)|We present an end-to-end computer vision pipeline to detect non-nutritive sucking (NNS) -- an infant sucking pattern with no nutrition delivered -- as a potential biomarker for developmental delays, using off-the-shelf baby monitor video footage. One barrier to clinical (or algorithmic) assessment of NNS stems from its sparsity, requiring experts to wade through hours of footage to find minutes of relevant activity. Our NNS activity segmentation algorithm solves this problem by identifying periods of NNS with high certainty -- up to 94.0\% average precision and 84.9\% average recall across 30 heterogeneous 60 s clips, drawn from our manually annotated NNS clinical in-crib dataset of 183 hours of overnight baby monitor footage from 19 infants. Our method is based on an underlying NNS action recognition algorithm, which uses spatiotemporal deep learning networks and infant-specific pose estimation, achieving 94.9\% accuracy in binary classification of 960 2.5 s balanced NNS vs. non-NNS clips. Tested on our second, independent, and public NNS in-the-wild dataset, NNS recognition classification reaches 92.3\% accuracy, and NNS segmentation achieves 90.8\% precision and 84.2\% recall.|

## ai

### neonatal+ai
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-16**|**MedFMC: A Real-world Dataset and Benchmark For Foundation Model Adaptation in Medical Image Classification**|Dequan Wang et.al.|[2306.09579v1](http://arxiv.org/abs/2306.09579v1)|null|Foundation models, often pre-trained with large-scale data, have achieved paramount success in jump-starting various vision and language applications. Recent advances further enable adapting foundation models in downstream tasks efficiently using only a few training samples, e.g., in-context learning. Yet, the application of such learning paradigms in medical image analysis remains scarce due to the shortage of publicly accessible data and benchmarks. In this paper, we aim at approaches adapting the foundation models for medical image classification and present a novel dataset and benchmark for the evaluation, i.e., examining the overall performance of accommodating the large-scale foundation models downstream on a set of diverse real-world clinical tasks. We collect five sets of medical imaging data from multiple institutes targeting a variety of real-world clinical tasks (22,349 images in total), i.e., thoracic diseases screening in X-rays, pathological lesion tissue screening, lesion detection in endoscopy images, neonatal jaundice evaluation, and diabetic retinopathy grading. Results of multiple baseline methods are demonstrated using the proposed dataset from both accuracy and cost-effective perspectives.|
|**2023-05-28**|**A joint estimation approach for monotonic regression functions in general dimensions**|Christian Rohrbeck et.al.|[2305.17711v1](http://arxiv.org/abs/2305.17711v1)|null|Regression analysis under the assumption of monotonicity is a well-studied statistical problem and has been used in a wide range of applications. However, there remains a lack of a broadly applicable methodology that permits information borrowing, for efficiency gains, when jointly estimating multiple monotonic regression functions. We introduce such a methodology by extending the isotonic regression problem presented in the article "The isotonic regression problem and its dual" (Barlow and Brunk, 1972). The presented approach can be applied to both fixed and random designs and any number of explanatory variables (regressors). Our framework penalizes pairwise differences in the values (levels) of the monotonic function estimates, with the weight of penalty being determined based on a statistical test, which results in information being shared across data sets if similarities in the regression functions exist. Function estimates are subsequently derived using an iterative optimization routine that uses existing solution algorithms for the isotonic regression problem. Simulation studies for normally and binomially distributed response data illustrate that function estimates are consistently improved if similarities between functions exist, and are not oversmoothed otherwise. We further apply our methodology to analyse two public health data sets: neonatal mortality data for Porto Alegre, Brazil, and stroke patient data for North West England.|
|**2023-05-02**|**Establishing a Learning Model for Correct Hand Hygiene Technique in a NICU**|Irén A. Kopcsóné Németh et.al.|[2305.01366v1](http://arxiv.org/abs/2305.01366v1)|null|The ability of healthcare workers to learn proper hand hygiene has been an understudied area of research. Generally, hand hygiene skills are regarded as a key contributor to reduce critical infections and healthcare-associated infections. In a clinical setup, at a Neonatal Intensive Care Unit (NICU), the outcome of a multi-modal training initiative was recorded, where objective feedback was provided to the staff. It was hypothesized that staff at the NICU are more sensitive towards applying increased patient safety measures. Outcomes were recorded as the ability to cover all hand surfaces with Alcohol-Based Handrub (ABHR), modelled as a time-series of measurements. The learning ability to rub in with 1.5 mL and with 3 mL was also assessed. As a secondary outcome, handrub consumption and infection numbers were recorded. It has been observed that some staff members were able to quickly learn the proper hand hygiene, even with the limited 1.5 mL, while others were not capable of acquiring the technique even with 3 mL. When analyzing the 1.5 mL group, it was deemed an insufficient ABHR amount, while with 3 mL, the critical necessity of skill training to achieve complete coverage was documented. Identifying these individuals helps the infection control staff to better focus their training efforts. The training led to a 157% increase in handrub consumption. The setting of the study did not allow to show a measurable reduction in the number of hospital infections. It has been concluded that the training method chosen by the staff greatly affects the quality of the outcomes.|
|**2023-04-26**|**ScatterFormer: Locally-Invariant Scattering Transformer for Patient-Independent Multispectral Detection of Epileptiform Discharges**|Ruizhe Zheng et.al.|[2304.14919v1](http://arxiv.org/abs/2304.14919v1)|[link](https://github.com/albertcheng19/scatterformer)|Patient-independent detection of epileptic activities based on visual spectral representation of continuous EEG (cEEG) has been widely used for diagnosing epilepsy. However, precise detection remains a considerable challenge due to subtle variabilities across subjects, channels and time points. Thus, capturing fine-grained, discriminative features of EEG patterns, which is associated with high-frequency textural information, is yet to be resolved. In this work, we propose Scattering Transformer (ScatterFormer), an invariant scattering transform-based hierarchical Transformer that specifically pays attention to subtle features. In particular, the disentangled frequency-aware attention (FAA) enables the Transformer to capture clinically informative high-frequency components, offering a novel clinical explainability based on visual encoding of multichannel EEG signals. Evaluations on two distinct tasks of epileptiform detection demonstrate the effectiveness our method. Our proposed model achieves median AUCROC and accuracy of 98.14%, 96.39% in patients with Rolandic epilepsy. On a neonatal seizure detection benchmark, it outperforms the state-of-the-art by 9% in terms of average AUCROC.|
|**2023-04-12**|**NRTS: A Client-Server architecture for supporting data recording, transmission and evaluation of multidisciplinary teams during the neonatal resuscitation simulation scenario**|Manuel Striani et.al.|[2304.09860v1](http://arxiv.org/abs/2304.09860v1)|null|In this technical report, we describe Neonatal Resuscitation Training Simulator (NRTS), an Android mobile app designed to support medical experts to input, transmit and record data during a High-Fidelity Simulation course for neonatal resuscitation. This mobile app allows one to automatically send all the recorded data from "Neonatal Intensive Care Unit" (NICU) of Casale Monferrato Children's Hospital, (Italy) to a server located at the Department of Science and Technological Innovation (DiSIT), University of Piemonte Orientale (Italy). Finally, the medical instructor can view statistics on a simulation exercise that may be used during the de-briefing phase for the evaluation of multidisciplinary teams involved in the simulation scenarios.|
|**2023-03-28**|**Predicting Adverse Neonatal Outcomes for Preterm Neonates with Multi-Task Learning**|Jingyang Lin et.al.|[2303.15656v1](http://arxiv.org/abs/2303.15656v1)|null|Diagnosis of adverse neonatal outcomes is crucial for preterm survival since it enables doctors to provide timely treatment. Machine learning (ML) algorithms have been demonstrated to be effective in predicting adverse neonatal outcomes. However, most previous ML-based methods have only focused on predicting a single outcome, ignoring the potential correlations between different outcomes, and potentially leading to suboptimal results and overfitting issues. In this work, we first analyze the correlations between three adverse neonatal outcomes and then formulate the diagnosis of multiple neonatal outcomes as a multi-task learning (MTL) problem. We then propose an MTL framework to jointly predict multiple adverse neonatal outcomes. In particular, the MTL framework contains shared hidden layers and multiple task-specific branches. Extensive experiments have been conducted using Electronic Health Records (EHRs) from 121 preterm neonates. Empirical results demonstrate the effectiveness of the MTL framework. Furthermore, the feature importance is analyzed for each neonatal outcome, providing insights into model interpretability.|
|**2023-03-21**|**The Multiscale Surface Vision Transformer**|Simon Dahan et.al.|[2303.11909v1](http://arxiv.org/abs/2303.11909v1)|[link](https://github.com/metrics-lab/surface-vision-transformers)|Surface meshes are a favoured domain for representing structural and functional information on the human cortex, but their complex topology and geometry pose significant challenges for deep learning analysis. While Transformers have excelled as domain-agnostic architectures for sequence-to-sequence learning, notably for structures where the translation of the convolution operation is non-trivial, the quadratic cost of the self-attention operation remains an obstacle for many dense prediction tasks. Inspired by some of the latest advances in hierarchical modelling with vision transformers, we introduce the Multiscale Surface Vision Transformer (MS-SiT) as a backbone architecture for surface deep learning. The self-attention mechanism is applied within local-mesh-windows to allow for high-resolution sampling of the underlying data, while a shifted-window strategy improves the sharing of information between windows. Neighbouring patches are successively merged, allowing the MS-SiT to learn hierarchical representations suitable for any prediction task. Results demonstrate that the MS-SiT outperforms existing surface deep learning methods for neonatal phenotyping prediction tasks using the Developing Human Connectome Project (dHCP) dataset. Furthermore, building the MS-SiT backbone into a U-shaped architecture for surface segmentation demonstrates competitive results on cortical parcellation using the UK Biobank (UKB) and manually-annotated MindBoggle datasets. Code and trained models are publicly available at https://github.com/metrics-lab/surface-vision-transformers .|
|**2023-03-14**|**Activity Recognition From Newborn Resuscitation Videos**|Øyvind Meinich-Bache et.al.|[2303.07789v1](http://arxiv.org/abs/2303.07789v1)|null|Objective: Birth asphyxia is one of the leading causes of neonatal deaths. A key for survival is performing immediate and continuous quality newborn resuscitation. A dataset of recorded signals during newborn resuscitation, including videos, has been collected in Haydom, Tanzania, and the aim is to analyze the treatment and its effect on the newborn outcome. An important step is to generate timelines of relevant resuscitation activities, including ventilation, stimulation, suction, etc., during the resuscitation episodes. Methods: We propose a two-step deep neural network system, ORAA-net, utilizing low-quality video recordings of resuscitation episodes to do activity recognition during newborn resuscitation. The first step is to detect and track relevant objects using Convolutional Neural Networks (CNN) and post-processing, and the second step is to analyze the proposed activity regions from step 1 to do activity recognition using 3D CNNs. Results: The system recognized the activities newborn uncovered, stimulation, ventilation and suction with a mean precision of 77.67 %, a mean recall of 77,64 %, and a mean accuracy of 92.40 %. Moreover, the accuracy of the estimated number of Health Care Providers (HCPs) present during the resuscitation episodes was 68.32 %. Conclusion: The results indicate that the proposed CNN-based two-step ORAAnet could be used for object detection and activity recognition in noisy low-quality newborn resuscitation videos. Significance: A thorough analysis of the effect the different resuscitation activities have on the newborn outcome could potentially allow us to optimize treatment guidelines, training, debriefing, and local quality improvement in newborn resuscitation.|
|**2023-02-14**|**Classification of Lung Pathologies in Neonates using Dual Tree Complex Wavelet Transform**|Sagarjit Aujla et.al.|[2302.07157v2](http://arxiv.org/abs/2302.07157v2)|null|Annually 8500 neonatal deaths are reported in the US due to respiratory failure. Recently, Lung Ultrasound (LUS), due to its radiation free nature, portability, and being cheaper is gaining wide acceptability as a diagnostic tool for lung conditions. However, lack of highly trained medical professionals has limited its use especially in remote areas. To address this, an automated screening system that captures characteristics of the LUS patterns can be of significant assistance to clinicians who are not experts in lung ultrasound (LUS) images. In this paper, we propose a feature extraction method designed to quantify the spatially-localized line patterns and texture patterns found in LUS images. Using the dual-tree complex wavelet transform (DTCWT) and four types of common image features we propose a method to classify the LUS images into 6 common neonatal lung conditions. These conditions are normal lung, pneumothorax (PTX), transient tachypnea of the newborn (TTN), respiratory distress syndrome (RDS), chronic lung disease (CLD) and consolidation (CON) that could be pneumonia or atelectasis. The proposed method using DTCWT decomposition extracted global statistical, grey-level co-occurrence matrix (GLCM), grey-level run length matrix (GLRLM) and linear binary pattern (LBP) features to be fed to a linear discriminative analysis (LDA) based classifier. Using 15 best DTCWT features along with 3 clinical features the proposed approach achieved a per-image classification accuracy of 92.78% with a balanced dataset containing 720 images from 24 patients and 74.39% with the larger unbalanced dataset containing 1550 images from 42 patients. Likewise, the proposed method achieved a maximum per-subject classification accuracy of 81.53% with 43 DTCWT features and 3 clinical features using the balanced dataset and 64.97% with 13 DTCWT features and 3 clinical features using the unbalanced dataset.|
|**2023-02-08**|**Neonatal Face and Facial Landmark Detection from Video Recordings**|Ethan Grooby et.al.|[2302.04341v1](http://arxiv.org/abs/2302.04341v1)|null|This paper explores automated face and facial landmark detection of neonates, which is an important first step in many video-based neonatal health applications, such as vital sign estimation, pain assessment, sleep-wake classification, and jaundice detection. Utilising three publicly available datasets of neonates in the clinical environment, 366 images (258 subjects) and 89 (66 subjects) were annotated for training and testing, respectively. Transfer learning was applied to two YOLO-based models, with input training images augmented with random horizontal flipping, photo-metric colour distortion, translation and scaling during each training epoch. Additionally, the re-orientation of input images and fusion of trained deep learning models was explored. Our proposed model based on YOLOv7Face outperformed existing methods with a mean average precision of 84.8% for face detection, and a normalised mean error of 0.072 for facial landmark detection. Overall, this will assist in the development of fully automated neonatal health assessment algorithms.|
|**2023-02-01**|**The Past, Current, and Future of Neonatal Intensive Care Units with Artificial Intelligence**|Elif Keles et.al.|[2302.00225v1](http://arxiv.org/abs/2302.00225v1)|null|Artificial intelligence (AI), specifically a branch of AI called deep learning (DL), has proven revolutionary developments in almost all fields, from computer vision to health sciences, and its effects in medicine have changed clinical applications significantly. Although some sub-fields of medicine such as pediatrics have been relatively slow in receiving critical benefits of AI, related research in pediatrics started to be accumulated to a significant level too. Hence, in this paper, we review recently developed machine learning and deep learning based systems for neonatology applications. We systematically evaluate the role of AI in neonatology applications, define the methodologies, including algorithmic developments, and describe the remaining challenges in neonatal diseases. To date, survival analysis, neuroimaging, EEG, pattern analysis of vital parameters, and retinopathy of prematurity diagnosis with AI have been the main focus in neonatology. We have categorically summarized 96 research articles, from 1996 to 2022, and discussed their pros and cons, respectively. We also discuss possible directions for new AI models and the future of neonatology with the rising power of AI, suggesting roadmaps for integration of AI into neonatal intensive care units.|
|**2022-12-26**|**Investigation of the aptness of newly developed epoxy-based equivalent tissues for newborn and 5-years old in paediatric radiology**|Nabeel Ibrahim Ashour et.al.|[2212.13002v1](http://arxiv.org/abs/2212.13002v1)|null|The varied radiological applications of tissue equivalent (TE) materials encompass quality checks, diagnostic imaging and dose evaluations. Nevertheless, the availability of compounds representative of paediatric patient tissues for scientific use in lower diagnostic photon energy spectra is limited. In this study, several TE substitutes were developed which replicate the radiographic characteristics of human tissue within these energy ranges, i.e. TE materials for neonatal soft tissue (ESST-NB), neonatal skeletal tissue (ESTB-NB), and the equivalent tissue types representative of a 5 year old child (ESST and ESBT, respectively). The ORNL stylised computational model series was used as a source for the desired elemental proportions. The density, effective atomic number, CT numbers and electron densities calculated for the developed tissue substitutes approximated those of the phantom system used as a reference. Additionally, in keeping with the material choice and production limitations, as close correlations as possible were achieved for all the materials in relation to the reference data for mass densities, mass attenuation coefficients and mass energy-absorption coefficients. The TE substitutes for the newborn over an energy range of 47 keV to 66 keV exhibited maximum discrepancies for {\mu}/\r{ho} of 1.6% to -3.01%, and for {\mu}_en/\r{ho} of 1.15% to -1.4% in relation to the ORNL reference samples. The respective equivalent data ranges were 1.09 % to -3.02% and 1.92% to -2.53% for the TE materials representative of a 5-year-old. Given the excellent concordance achieved between the newly constructed TE materials and the reference data, these compounds can subsequently be utilised to create physical phantoms representative of tissue types in neonates and children aged 5 years.|
|**2022-12-12**|**Estimating the timing of stillbirths in countries worldwide using a Bayesian hierarchical penalized splines regression model**|Michael Y. C. Chong et.al.|[2212.06219v1](http://arxiv.org/abs/2212.06219v1)|null|Reducing the global burden of stillbirths is important to improving child and maternal health. Of interest is understanding patterns in the timing of stillbirths -- that is, whether they occur in the intra- or antepartum period -- because stillbirths that occur intrapartum are largely preventable. However, data availability on the timing of stillbirths is highly variable across the world, with low- and middle-income countries generally having few reliable observations. In this paper we develop a Bayesian penalized splines regression framework to estimate the proportion of stillbirths that are intrapartum for all countries worldwide. The model accounts for known relationships with neonatal mortality, pools information across geographic regions, incorporates different errors based on data attributes, and allows for data-driven temporal trends. A weighting procedure is proposed to account for unrepresentative subnational data. Results suggest that the intrapartum proportion is generally decreasing over time, but progress is slower in some regions, particularly Sub-Saharan Africa.|
|**2022-12-05**|**Robust multiple method comparison and transformation**|Florian Dufey et.al.|[2212.02306v1](http://arxiv.org/abs/2212.02306v1)|null|A generalization of Passing-Bablok regression for the simultaneous comparison of multiple methods is proposed. Possible applications include assay migration studies or interlaboratory trials. The method is shown to reduce to the usual Passing--Bablok estimator if only two methods are compared. It is close in spirit to reduced major axis regression, which is, however, not robust. To obtain a robust estimator, the major axis is replaced by the (hyper-)spherical median axis. The method is applied to the comparison of SARS-CoV-2 serological tests, bilirubin in neonates, and to a clinical test using different instruments, sample preparations and reagent lots. Plots, similar to the well known Bland-Altman plots, are developed for a posteriori checks of the assumed variance structure.|
|**2022-11-16**|**Neurodevelopmental Phenotype Prediction: A State-of-the-Art Deep Learning Model**|Dániel Unyi et.al.|[2211.08831v1](http://arxiv.org/abs/2211.08831v1)|[link](https://github.com/daniel-unyi-42/neurodevelopmental-phenotype-prediction)|A major challenge in medical image analysis is the automated detection of biomarkers from neuroimaging data. Traditional approaches, often based on image registration, are limited in capturing the high variability of cortical organisation across individuals. Deep learning methods have been shown to be successful in overcoming this difficulty, and some of them have even outperformed medical professionals on certain datasets. In this paper, we apply a deep neural network to analyse the cortical surface data of neonates, derived from the publicly available Developing Human Connectome Project (dHCP). Our goal is to identify neurodevelopmental biomarkers and to predict gestational age at birth based on these biomarkers. Using scans of preterm neonates acquired around the term-equivalent age, we were able to investigate the impact of preterm birth on cortical growth and maturation during late gestation. Besides reaching state-of-the-art prediction accuracy, the proposed model has much fewer parameters than the baselines, and its error stays low on both unregistered and registered cortical surfaces.|
|**2022-11-08**|**Automated CFD shape optimization of stator blades for the PediaFlow pediatric ventricular assist device**|Mansur Zhussupbekov et.al.|[2211.04401v1](http://arxiv.org/abs/2211.04401v1)|null|PediaFlow is a miniature mixed-flow ventricular assist device for neonates and toddlers. PediaFlow has a fully magnetically levitated rotor which improves biocompatibility, but the increased length of the rotor creates a long annular passage where fluid energy is lost. Therefore, a set of helical stator blades was proposed immediately after the impeller stage to remove the swirling flow and recover the dynamic head as static pressure. Automated computational fluid dynamics (CFD) shape optimization of the stator blades was performed to maximize pressure recovery at the operating point of 1.5 LPM and 16,000 RPM. Additionally, the effect on hemolysis and thrombogenicity was assessed using numerical modeling. The optimization algorithm favored fewer blades of greater length over a larger number of short blades. The ratio of wrap angle to axial length emerged as a key constraint to ensure the viability of a design. The best design had 2 blades and generated 73 mmHg of pressure recovery in an isolated stage. When re-introduced to the CFD simulation of the complete flow path, the added stator stage increased the pump head by 46% and improved the pump efficiency from 21.9% to 25.7% at the selected operating point. Automated CFD shape optimization combined with in silico evaluation of hemocompatibility can be an effective tool for exploring design choices and informing early development process.|
|**2022-09-20**|**Unresolved excess accumulation of myelin-derived cholesterol contributes to scar formation after spinal cord injury**|Bolin Zheng et.al.|[2209.09700v1](http://arxiv.org/abs/2209.09700v1)|null|Background: Spinal cord injury triggers complex pathological cascades, resulting in destructive tissue damage and incomplete tissue repair. Scar formation is generally considered as a barrier for regeneration in central nervous system (CNS), while the intrinsic mechanism of scar-forming after spinal cord injury has not been completed deciphered. Methods: We assessed cholesterol hemostasis in spinal cord lesions and injured peripheral nerves using confocal reflection microscopy and real-time PCR analyses. The involvement of the proteins, which were predicted to promote cholesterol efflux in spinal cord lesions, were assessed with Liver X receptor (LXR) agonist and Apolipoprotein E (APOE) deficiency. The role of reverse cholesterol transport (RCT) in cholesterol clearance was examined in APOE KO mice injured sciatic nerves and myelin-overloaded macrophages in vitro. Finally, we determined the consequence of excess cholesterol accumulation in CNS by transplantation of myelin into neonatal spinal cord lesions. Results: We found that excess cholesterol accumulates in phagocytes and is inefficiently removed in spinal cord lesions in young-adult mice. Interestingly, we observed that excessive cholesterol also accumulates in injured peripheral nerves, but is subsequently removed by RCT. Meanwhile, preventing RCT led to macrophage accumulation and fibrosis in injured peripheral nerves. Furthermore, the neonatal mouse spinal cord lesions are devoid of myelin-derived lipids, and able to heal without excess cholesterol accumulation. We found that transplantation of myelin into neonatal lesions disrupts healing with excessive cholesterol accumulation, persistent macrophage activation and fibrosis, indicating myelin-derived cholesterol plays a critical role in impaired wound healing.|
|**2022-08-29**|**Slice estimation in diffusion MRI of neonatal and fetal brains in image and spherical harmonics domains using autoencoders**|Hamza Kebiri et.al.|[2208.13328v1](http://arxiv.org/abs/2208.13328v1)|null|Diffusion MRI (dMRI) of the developing brain can provide valuable insights into the white matter development. However, slice thickness in fetal dMRI is typically high (i.e., 3-5 mm) to freeze the in-plane motion, which reduces the sensitivity of the dMRI signal to the underlying anatomy. In this study, we aim at overcoming this problem by using autoencoders to learn unsupervised efficient representations of brain slices in a latent space, using raw dMRI signals and their spherical harmonics (SH) representation. We first learn and quantitatively validate the autoencoders on the developing Human Connectome Project pre-term newborn data, and further test the method on fetal data. Our results show that the autoencoder in the signal domain better synthesized the raw signal. Interestingly, the fractional anisotropy and, to a lesser extent, the mean diffusivity, are best recovered in missing slices by using the autoencoder trained with SH coefficients. A comparison was performed with the same maps reconstructed using an autoencoder trained with raw signals, as well as conventional interpolation methods of raw signals and SH coefficients. From these results, we conclude that the recovery of missing/corrupted slices should be performed in the signal domain if the raw signal is aimed to be recovered, and in the SH domain if diffusion tensor properties (i.e., fractional anisotropy) are targeted. Notably, the trained autoencoders were able to generalize to fetal dMRI data acquired using a much smaller number of diffusion gradients and a lower b-value, where we qualitatively show the consistency of the estimated diffusion tensor maps.|
|**2022-08-25**|**Development of Sleep State Trend (SST), a bedside measure of neonatal sleep state fluctuations based on single EEG channels**|Saeed Montazeri Moghadam et.al.|[2208.11933v1](http://arxiv.org/abs/2208.11933v1)|null|Objective: To develop and validate an automated method for bedside monitoring of sleep state fluctuations in neonatal intensive care units.   Methods: A deep learning -based algorithm was designed and trained using 53 EEG recordings from a long-term (a)EEG monitoring in 30 near-term neonates. The results were validated using an external dataset from 30 polysomnography recordings. In addition to training and validating a single EEG channel quiet sleep detector, we constructed Sleep State Trend (SST), a bedside-ready means for visualizing classifier outputs.   Results: The accuracy of quiet sleep detection in the training data was 90%, and the accuracy was comparable (85-86%) in all bipolar derivations available from the 4-electrode recordings. The algorithm generalized well to an external dataset, showing 81% overall accuracy despite different signal derivations. SST allowed an intuitive, clear visualization of the classifier output.   Conclusions: Fluctuations in sleep states can be detected at high fidelity from a single EEG channel, and the results can be visualized as a transparent and intuitive trend in the bedside monitors.   Significance: The Sleep State Trend (SST) may provide caregivers a real-time view of sleep state fluctuations and its cyclicity.|
|**2022-08-23**|**Regression discontinuity design in perinatal epidemiology and birth cohort research**|Maja Popovic et.al.|[2208.11047v1](http://arxiv.org/abs/2208.11047v1)|null|Regression discontinuity design (RDD) is a quasi-experimental approach to study the causal effects of an intervention/treatment on later health outcomes. It exploits a continuously measured assignment variable with a clearly defined cut-off above or below which the population is at least partially assigned to the intervention/treatment. We describe the RDD and outline the applications of RDD in the context of perinatal epidemiology and birth cohort research.   There is an increasing number of studies using RDD in perinatal and pediatric epidemiology. Most of these studies were conducted in the context of education, social and welfare policies, healthcare organization, insurance, and preventive programs. Additional thematic fields include clinically relevant research questions, shock events, social and environmental factors, and changes in guidelines. Maternal and perinatal characteristics, such as age, birth weight and gestational age are frequently used assignment variables to study the effects of the type and intensity of neonatal care, health insurance, and supplemental newborn benefits. Different socioeconomic measures have been used to study the effects of social, welfare and cash transfer programs, while age or date of birth served as assignment variables to study the effects of vaccination programs, pregnancy-specific guidelines, maternity and paternity leave policies and introduction of newborn-based welfare programs.   RDD has advantages, including relatively weak and testable assumptions, strong internal validity, intuitive interpretation, and transparent and simple graphical representation. However, its use in birth cohort research is hampered by the rarity of settings outside of policy and program evaluations, low statistical power, limited external validity (geographic- and time-specific settings) and potential contamination by other exposures/interventions.|
|**2022-07-25**|**Deep learning based non-contact physiological monitoring in Neonatal Intensive Care Unit**|Nicky Nirlipta Sahoo et.al.|[2207.11886v1](http://arxiv.org/abs/2207.11886v1)|null|Preterm babies in the Neonatal Intensive Care Unit (NICU) have to undergo continuous monitoring of their cardiac health. Conventional monitoring approaches are contact-based, making the neonates prone to various nosocomial infections. Video-based monitoring approaches have opened up potential avenues for contactless measurement. This work presents a pipeline for remote estimation of cardiopulmonary signals from videos in NICU setup. We have proposed an end-to-end deep learning (DL) model that integrates a non-learning based approach to generate surrogate ground truth (SGT) labels for supervision, thus refraining from direct dependency on true ground truth labels. We have performed an extended qualitative and quantitative analysis to examine the efficacy of our proposed DL-based pipeline and achieved an overall average mean absolute error of 4.6 beats per minute (bpm) and root mean square error of 6.2 bpm in the estimated heart rate.|
|**2022-07-18**|**Superficial White Matter Analysis: An Efficient Point-cloud-based Deep Learning Framework with Supervised Contrastive Learning for Consistent Tractography Parcellation across Populations and dMRI Acquisitions**|Tengfei Xue et.al.|[2207.08975v3](http://arxiv.org/abs/2207.08975v3)|null|Diffusion MRI tractography is an advanced imaging technique that enables in vivo mapping of the brain's white matter connections. White matter parcellation classifies tractography streamlines into clusters or anatomically meaningful tracts. It enables quantification and visualization of whole-brain tractography. Currently, most parcellation methods focus on the deep white matter (DWM), whereas fewer methods address the superficial white matter (SWM) due to its complexity. We propose a novel two-stage deep-learning-based framework, Superficial White Matter Analysis (SupWMA), that performs an efficient and consistent parcellation of 198 SWM clusters from whole-brain tractography. A point-cloud-based network is adapted to our SWM parcellation task, and supervised contrastive learning enables more discriminative representations between plausible streamlines and outliers for SWM. We train our model on a large-scale tractography dataset including streamline samples from labeled long- and medium-range (over 40 mm) SWM clusters and anatomically implausible streamline samples, and we perform testing on six independently acquired datasets of different ages and health conditions (including neonates and patients with space-occupying brain tumors). Compared to several state-of-the-art methods, SupWMA obtains highly consistent and accurate SWM parcellation results on all datasets, showing good generalization across the lifespan in health and disease. In addition, the computational speed of SupWMA is much faster than other methods.|
|**2022-07-14**|**Spatial Aggregation with Respect to a Population Distribution**|John Paige et.al.|[2207.06700v1](http://arxiv.org/abs/2207.06700v1)|[link](https://github.com/paigejo/summer)|Spatial aggregation with respect to a population distribution involves estimating aggregate quantities for a population based on an observation of individuals in a subpopulation. In this context, a geostatistical workflow must account for three major sources of `aggregation error': aggregation weights, fine scale variation, and finite population variation. However, common practice is to treat the unknown population distribution as a known population density and ignore empirical variability in outcomes. We improve common practice by introducing a `sampling frame model' that allows aggregation models to account for the three sources of aggregation error simply and transparently.   We compare the proposed and the traditional approach using two simulation studies that mimic neonatal mortality rate (NMR) data from the 2014 Kenya Demographic and Health Survey (KDHS2014). For the traditional approach, undercoverage/overcoverage depends arbitrarily on the aggregation grid resolution, while the new approach exhibits low sensitivity. The differences between the two aggregation approaches increase as the population of an area decreases. The differences are substantial at the second administrative level and finer, but also at the first administrative level for some population quantities. We find differences between the proposed and traditional approach are consistent with those we observe in an application to NMR data from the KDHS2014.|
|**2022-06-27**|**ContraReg: Contrastive Learning of Multi-modality Unsupervised Deformable Image Registration**|Neel Dey et.al.|[2206.13434v1](http://arxiv.org/abs/2206.13434v1)|null|Establishing voxelwise semantic correspondence across distinct imaging modalities is a foundational yet formidable computer vision task. Current multi-modality registration techniques maximize hand-crafted inter-domain similarity functions, are limited in modeling nonlinear intensity-relationships and deformations, and may require significant re-engineering or underperform on new tasks, datasets, and domain pairs. This work presents ContraReg, an unsupervised contrastive representation learning approach to multi-modality deformable registration. By projecting learned multi-scale local patch features onto a jointly learned inter-domain embedding space, ContraReg obtains representations useful for non-rigid multi-modality alignment. Experimentally, ContraReg achieves accurate and robust results with smooth and invertible deformations across a series of baselines and ablations on a neonatal T1-T2 brain MRI registration task with all methods validated over a wide range of deformation regularization strengths.|
|**2022-06-20**|**PhaTYP: Predicting the lifestyle for bacteriophages using BERT**|Jiayu Shang et.al.|[2206.09693v1](http://arxiv.org/abs/2206.09693v1)|[link](https://github.com/kennthshang/phatyp)|Bacteriophages (or phages), which infect bacteria, have two distinct lifestyles: virulent and temperate. Predicting the lifestyle of phages helps decipher their interactions with their bacterial hosts, aiding phages' applications in fields such as phage therapy. Because experimental methods for annotating the lifestyle of phages cannot keep pace with the fast accumulation of sequenced phages, computational method for predicting phages' lifestyles has become an attractive alternative. Despite some promising results, computational lifestyle prediction remains difficult because of the limited known annotations and the sheer amount of sequenced phage contigs assembled from metagenomic data. In particular, most of the existing tools cannot precisely predict phages' lifestyles for short contigs. In this work, we develop PhaTYP (Phage TYPe prediction tool) to improve the accuracy of lifestyle prediction on short contigs. We design two different training tasks, self-supervised and fine-tuning tasks, to overcome lifestyle prediction difficulties. We rigorously tested and compared PhaTYP with four state-of-the-art methods: DeePhage, PHACTS, PhagePred, and BACPHLIP. The experimental results show that PhaTYP outperforms all these methods and achieves more stable performance on short contigs. In addition, we demonstrated the utility of PhaTYP for analyzing the phage lifestyle on human neonates' gut data. This application shows that PhaTYP is a useful means for studying phages in metagenomic data and helps extend our understanding of microbial communities.|
|**2022-06-15**|**A Deep Generative Model of Neonatal Cortical Surface Development**|Abdulah Fawaz et.al.|[2206.07542v2](http://arxiv.org/abs/2206.07542v2)|null|The neonatal cortical surface is known to be affected by preterm birth, and the subsequent changes to cortical organisation have been associated with poorer neurodevelopmental outcomes. Deep Generative models have the potential to lead to clinically interpretable models of disease, but developing these on the cortical surface is challenging since established techniques for learning convolutional filters are inappropriate on non-flat topologies. To close this gap, we implement a surface-based CycleGAN using mixture model CNNs (MoNet) to translate sphericalised neonatal cortical surface features (curvature and T1w/T2w cortical myelin) between different stages of cortical maturity. Results show our method is able to reliably predict changes in individual patterns of cortical organisation at later stages of gestation, validated by comparison to longitudinal data; and translate appearance between preterm and term gestation (> 37 weeks gestation), validated through comparison with a trained term/preterm classifier. Simulated differences in cortical maturation are consistent with observations in the literature.|
|**2022-06-09**|**Neonatal EEG graded for severity of background abnormalities in hypoxic-ischaemic encephalopathy**|John M O'Toole et.al.|[2206.04420v2](http://arxiv.org/abs/2206.04420v2)|[link](https://github.com/otoolej/downsample_open_eeg)|This report describes a set of neonatal electroencephalogram (EEG) recordings graded according to the severity of abnormalities in the background pattern. The dataset consists of 169 hours of multichannel EEG from 53 neonates recorded in a neonatal intensive care unit. All neonates received a diagnosis of hypoxic-ischaemic encephalopathy (HIE), the most common cause of brain injury in full term infants. For each neonate, multiple 1-hour epochs of good quality EEG were selected and then graded for background abnormalities. The grading system assesses EEG attributes such as amplitude and frequency, continuity, sleep--wake cycling, symmetry and synchrony, and abnormal waveforms. Background severity was then categorised into 4 grades: normal or mildly abnormal EEG, moderately abnormal EEG, severely abnormal EEG, and inactive EEG. The data can be used as a reference set of multi-channel EEG for neonates with HIE, for EEG training purposes, or for developing and evaluating automated grading algorithms.|
|**2022-05-16**|**An automatic pipeline for atlas-based fetal and neonatal brain segmentation and analysis**|Urru et.al.|[2205.07575v1](http://arxiv.org/abs/2205.07575v1)|[link](https://github.com/urrand/perinatal-pipeline)|The automatic segmentation of perinatal brain structures in magnetic resonance imaging (MRI) is of utmost importance for the study of brain growth and related complications. While different methods exist for adult and pediatric MRI data, there is a lack for automatic tools for the analysis of perinatal imaging. In this work, a new pipeline for fetal and neonatal segmentation has been developed. We also report the creation of two new fetal atlases, and their use within the pipeline for atlas-based segmentation, based on novel registration methods. The pipeline is also able to extract cortical and pial surfaces and compute features, such as curvature, thickness, sulcal depth, and local gyrification index. Results show that the introduction of the new templates together with our segmentation strategy leads to accurate results when compared to expert annotations, as well as better performances when compared to a reference pipeline (developing Human Connectome Project (dHCP)), for both early and late-onset fetal brains.|
|**2022-04-11**|**Ensemble learning using individual neonatal data for seizure detection**|Ana Borovac et.al.|[2204.07043v2](http://arxiv.org/abs/2204.07043v2)|[link](https://github.com/anaborovac/distributed-nsda)|Sharing medical data between institutions is difficult in practice due to data protection laws and official procedures within institutions. Therefore, most existing algorithms are trained on relatively small electroencephalogram (EEG) data sets which is likely to be detrimental to prediction accuracy. In this work, we simulate a case when the data can not be shared by splitting the publicly available data set into disjoint sets representing data in individual institutions. We propose to train a (local) detector in each institution and aggregate their individual predictions into one final prediction. Four aggregation schemes are compared, namely, the majority vote, the mean, the weighted mean and the Dawid-Skene method. The method was validated on an independent data set using only a subset of EEG channels. The ensemble reaches accuracy comparable to a single detector trained on all the data when sufficient amount of data is available in each institution. The weighted mean aggregation scheme showed best performance, it was only marginally outperformed by the Dawid--Skene method when local detectors approach performance of a single detector trained on all available data.|
|**2022-04-08**|**NeoRS: a neonatal resting state fMRI data preprocessing pipeline**|V. Enguix et.al.|[2204.05137v1](http://arxiv.org/abs/2204.05137v1)|[link](https://github.com/venguix/neors)|Resting state fMRI (rsfMRI) has been shown to be a promising tool to study intrinsic functional connectivity and assess its integrity in cerebral development. In neonates, where fMRI is limited to few paradigms, rsfMRI was shown to be a relevant tool to explore regional interactions of brain networks. However, to identify the resting state networks, data needs to be carefully processed. Because of the non-collaborative nature of the neonates, the differences in brain size and the reversed contrast compared to adults, neonates can't be processed with the existing adult pipelines. Therefore, we developed NeoRS. The main processing steps include atlas registration, skull tripping, segmentation, slice timing and head motion correction and confounds regression. To address the specificity of neonatal brain imaging, particular attention was given to registration including neonatal atlas type and parameters, such as brain size variations, and contrast differences compared to adults. Furthermore, head motion was scrutinized and optimized, as it is a major issue when processing neonatal data. The pipeline includes visual quality control assessment checkpoints. To assess its effectiveness, we used the data from the Baby Connectome Project including 10 neonates. NeoRS was designed to work on both multi-band and single-band acquisitions and is applicable on smaller datasets. It also includes popular functional connectivity analysis features such as seed based correlations. Language, default mode, dorsal attention, visual, ventral attention, motor and fronto parietal networks were evaluated. The different analyzed networks were in agreement with previously published studies in the neonate. NeoRS is coded in Matlab, it is open-source and available on https://github.com/venguix/NeoRS. NeoRS allows robust image processing of the neonatal rsfMRI data that can be readily customized to different datasets.|

### infant+ai
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-20**|**Deep Learning Methods for Retinal Blood Vessel Segmentation: Evaluation on Images with Retinopathy of Prematurity**|Gorana Gojić et.al.|[2306.11576v1](http://arxiv.org/abs/2306.11576v1)|null|Automatic blood vessel segmentation from retinal images plays an important role in the diagnosis of many systemic and eye diseases, including retinopathy of prematurity. Current state-of-the-art research in blood vessel segmentation from retinal images is based on convolutional neural networks. The solutions proposed so far are trained and tested on images from a few available retinal blood vessel segmentation datasets, which might limit their performance when given an image with retinopathy of prematurity signs. In this paper, we evaluate the performance of three high-performing convolutional neural networks for retinal blood vessel segmentation in the context of blood vessel segmentation on retinopathy of prematurity retinal images. The main motive behind the study is to test if existing public datasets suffice to develop a high-performing predictor that could assist an ophthalmologist in retinopathy of prematurity diagnosis. To do so, we create a dataset consisting solely of retinopathy of prematurity images with retinal blood vessel annotations manually labeled by two observers, where one is the ophthalmologist experienced in retinopathy of prematurity treatment. Experimental results show that all three solutions have difficulties in detecting the retinal blood vessels of infants due to a lower contrast compared to images from public datasets as demonstrated by a significant drop in classification sensitivity. All three solutions segment alongside retinal also choroidal blood vessels which are not used to diagnose retinopathy of prematurity, but instead represent noise and are confused with retinal blood vessels. By visual and numerical observations, we observe that existing solutions for retinal blood vessel segmentation need improvement toward more detailed datasets or deeper models in order to assist the ophthalmologist in retinopathy of prematurity diagnosis.|
|**2023-06-14**|**The generalized hyperbolic family and automatic model selection through the multiple-choice LASSO**|Luca Bagnato et.al.|[2306.08692v1](http://arxiv.org/abs/2306.08692v1)|null|We revisit the generalized hyperbolic (GH) distribution and its nested models. These include widely used parametric choices like the multivariate normal, skew-t, Laplace, and several others. We also introduce the multiple-choice LASSO, a novel penalized method for choosing among alternative constraints on the same parameter. A hierarchical multiple-choice LASSO penalized likelihood is optimized to perform simultaneous model selection and inference within the GH family. We illustrate our approach through a simulation study and a real data example about pre-term infants. The methodology proposed in this paper has been implemented in R functions which are available as supplementary material.|
|**2023-06-13**|**Overfitting Affects the Reliability of Radial Velocity Mass Estimates of the V1298 Tau Planets**|Sarah Blunt et.al.|[2306.08145v1](http://arxiv.org/abs/2306.08145v1)|[link](https://github.com/sblunt/v1298tauri)|Mass, radius, and age measurements of young (<100 Myr) planets have the power to shape our understanding of planet formation. However, young stars tend to be extremely variable in both photometry and radial velocity, which makes constraining these properties challenging. The V1298 Tau system of four ~0.5 Rjup planets transiting a pre-main sequence star presents an important, if stress-inducing, opportunity to directly observe and measure the properties of infant planets. Su\'arez-Mascare\~no et al. (2021) published radial-velocity-derived masses for two of the V1298 Tau planets using a state-of-the-art Gaussian Process regression framework. The planetary densities computed from these masses were surprisingly high, implying extremely rapid contraction after formation in tension with most existing planet formation theories. In an effort to further constrain the masses of the V1298 Tau planets, we obtained 36 RVs using Keck/HIRES, and analyzed them in concert with published RVs and photometry. Through performing a suite of cross validation tests, we found evidence that the preferred model of SM21 suffers from overfitting, defined as the inability to predict unseen data, rendering the masses unreliable. We detail several potential causes of this overfitting, many of which may be important for other RV analyses of other active stars, and recommend that additional time and resources be allocated to understanding and mitigating activity in active young stars such as V1298 Tau.|
|**2023-06-09**|**Two Independent Teachers are Better Role Model**|Afifa Khaled et.al.|[2306.05745v1](http://arxiv.org/abs/2306.05745v1)|[link](https://github.com/AfifaKhaled/Two-Independent-Teachers-are-Better-Role-Model)|Recent deep learning models have attracted substantial attention in infant brain analysis. These models have performed state-of-the-art performance, such as semi-supervised techniques (e.g., Temporal Ensembling, mean teacher). However, these models depend on an encoder-decoder structure with stacked local operators to gather long-range information, and the local operators limit the efficiency and effectiveness. Besides, the $MRI$ data contain different tissue properties ($TPs$) such as $T1$ and $T2$. One major limitation of these models is that they use both data as inputs to the segment process, i.e., the models are trained on the dataset once, and it requires much computational and memory requirements during inference. In this work, we address the above limitations by designing a new deep-learning model, called 3D-DenseUNet, which works as adaptable global aggregation blocks in down-sampling to solve the issue of spatial information loss. The self-attention module connects the down-sampling blocks to up-sampling blocks, and integrates the feature maps in three dimensions of spatial and channel, effectively improving the representation potential and discriminating ability of the model. Additionally, we propose a new method called Two Independent Teachers ($2IT$), that summarizes the model weights instead of label predictions. Each teacher model is trained on different types of brain data, $T1$ and $T2$, respectively. Then, a fuse model is added to improve test accuracy and enable training with fewer parameters and labels compared to the Temporal Ensembling method without modifying the network architecture. Empirical results demonstrate the effectiveness of the proposed method.|
|**2023-06-08**|**Design of Sturm global attractors 2: Time-reversible Chafee-Infante lattices of 3-nose meanders**|Bernold Fiedler et.al.|[2306.05232v1](http://arxiv.org/abs/2306.05232v1)|null|This sequel continues our exploration arxiv:2302.12531 of a deceptively ``simple'' class of global attractors, called Sturm due to nodal properties. They arise for the semilinear scalar parabolic PDE   \begin{equation}\label{eq:*}   u_t = u_{xx} + f(x,u,u_x) \tag{$*$}   \end{equation} on the unit interval $0 < x<1$, under Neumann boundary conditions. This models the interplay of reaction, advection, and diffusion.   Our classification is based on the Sturm meanders, which arise from a shooting approach to the ODE boundary value problem of equilibrium solutions $u=v(x)$. Specifically, we address meanders with only three ``noses'', each of which is innermost to a nested family of upper or lower meander arcs. The Chafee-Infante paradigm of 1974, with cubic nonlinearity $f=f(u)$, features just two noses.   We present, and fully prove, a precise description of global PDE connection graphs, graded by Morse index, for such gradient-like Morse-Smale systems \eqref{eq:*}. The directed edges denote PDE heteroclinic orbits $v_1 \leadsto v_2$ between equilibrium vertices $v_1, v_2$ of adjacent Morse index. The connection graphs can be described as a lattice-like structure of Chafee-Infante subgraphs. However, this simple description requires us to adjoin a single ``equilibrium'' vertex, formally, at Morse level -1. Surprisingly, for parabolic PDEs based on irreversible diffusion, the connection graphs then also exhibit global time reversibility.|
|**2023-06-02**|**BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models**|Marvin Lavechin et.al.|[2306.01506v2](http://arxiv.org/abs/2306.01506v2)|[link](https://github.com/marvinlvn/babyslm)|Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.|
|**2023-06-01**|**Generalizability analyses with a partially nested trial design: the Necrotizing Enterocolitis Surgery Trial**|Sarah E. Robertson et.al.|[2306.00855v1](http://arxiv.org/abs/2306.00855v1)|null|We discuss generalizability analyses under a partially nested trial design, where part of the trial is nested within a cohort of trial-eligible individuals, while the rest of the trial is not nested. This design arises, for example, when only some centers participating in a trial are able to collect data on non-randomized individuals, or when data on non-randomized individuals cannot be collected for the full duration of the trial. Our work is motivated by the Necrotizing Enterocolitis Surgery Trial (NEST) that compared initial laparotomy versus peritoneal drain for infants with necrotizing enterocolitis or spontaneous intestinal perforation. During the first phase of the study, data were collected from randomized individuals as well as consenting non-randomized individuals; during the second phase of the study, however, data were only collected from randomized individuals, resulting in a partially nested trial design. We propose methods for generalizability analyses with partially nested trial designs. We describe identification conditions and propose estimators for causal estimands in the target population of all trial-eligible individuals, both randomized and non-randomized, in the part of the data where the trial is nested, while using trial information spanning both parts. We evaluate the estimators in a simulation study.|
|**2023-05-25**|**Emergence of a phonological bias in ChatGPT**|Juan Manuel Toro et.al.|[2305.15929v2](http://arxiv.org/abs/2305.15929v2)|null|Current large language models, such as OpenAI's ChatGPT, have captured the public's attention because how remarkable they are in the use of language. Here, I demonstrate that ChatGPT displays phonological biases that are a hallmark of human language processing. More concretely, just like humans, ChatGPT has a consonant bias. That is, the chatbot has a tendency to use consonants over vowels to identify words. This is observed across languages that differ in their relative distribution of consonants and vowels such as English and Spanish. Despite the differences in how current artificial intelligence language models are trained to process linguistic stimuli and how human infants acquire language, such training seems to be enough for the emergence of a phonological bias in ChatGPT|
|**2023-05-24**|**From Interactive to Co-Constructive Task Learning**|Anna-Lisa Vollmer et.al.|[2305.15535v1](http://arxiv.org/abs/2305.15535v1)|null|Humans have developed the capability to teach relevant aspects of new or adapted tasks to a social peer with very few task demonstrations by making use of scaffolding strategies that leverage prior knowledge and importantly prior joint experience to yield a joint understanding and a joint execution of the required steps to solve the task. This process has been discovered and analyzed in parent-infant interaction and constitutes a ``co-construction'' as it allows both, the teacher and the learner, to jointly contribute to the task. We propose to focus research in robot interactive learning on this co-construction process to enable robots to learn from non-expert users in everyday situations. In the following, we will review current proposals for interactive task learning and discuss their main contributions with respect to the entailing interaction. We then discuss our notion of co-construction and summarize research insights from adult-child and human-robot interactions to elucidate its nature in more detail. From this overview we finally derive research desiderata that entail the dimensions architecture, representation, interaction and explainability.|
|**2023-05-22**|**Developmental Curiosity and Social Interaction in Virtual Agents**|Chris Doyle et.al.|[2305.13396v1](http://arxiv.org/abs/2305.13396v1)|null|Infants explore their complex physical and social environment in an organized way. To gain insight into what intrinsic motivations may help structure this exploration, we create a virtual infant agent and place it in a developmentally-inspired 3D environment with no external rewards. The environment has a virtual caregiver agent with the capability to interact contingently with the infant agent in ways that resemble play. We test intrinsic reward functions that are similar to motivations that have been proposed to drive exploration in humans: surprise, uncertainty, novelty, and learning progress. These generic reward functions lead the infant agent to explore its environment and discover the contingencies that are embedded into the caregiver agent. The reward functions that are proxies for novelty and uncertainty are the most successful in generating diverse experiences and activating the environment contingencies. We also find that learning a world model in the presence of an attentive caregiver helps the infant agent learn how to predict scenarios with challenging social and physical dynamics. Taken together, our findings provide insight into how curiosity-like intrinsic rewards and contingent social interaction lead to dynamic social behavior and the creation of a robust predictive world model.|
|**2023-05-21**|**Towards Robust Family-Infant Audio Analysis Based on Unsupervised Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio**|Jialu Li et.al.|[2305.12530v2](http://arxiv.org/abs/2305.12530v2)|null|To perform automatic family audio analysis, past studies have collected recordings using phone, video, or audio-only recording devices like LENA, investigated supervised learning methods, and used or fine-tuned general-purpose embeddings learned from large pretrained models. In this study, we advance the audio component of a new infant wearable multi-modal device called LittleBeats (LB) by learning family audio representation via wav2vec 2.0 (W2V2) pertaining. We show given a limited number of labeled LB home recordings, W2V2 pretrained using 1k-hour of unlabeled home recordings outperforms oracle W2V2 pretrained on 52k-hour unlabeled audio in terms of parent/infant speaker diarization (SD) and vocalization classifications (VC) at home. Extra relevant external unlabeled and labeled data further benefit W2V2 pretraining and fine-tuning. With SpecAug and environmental speech corruptions, we obtain 12% relative gain on SD and moderate boost on VC. Code and model weights are available.|
|**2023-05-18**|**Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses**|Eliza Kosoy et.al.|[2305.11243v1](http://arxiv.org/abs/2305.11243v1)|null|Developmental psychologists have spent decades devising experiments to test the intelligence and knowledge of infants and children, tracing the origin of crucial concepts and capacities. Moreover, experimental techniques in developmental psychology have been carefully designed to discriminate the cognitive capacities that underlie particular behaviors. We propose that using classical experiments from child development is a particularly effective way to probe the computational abilities of AI models, in general, and LLMs in particular. First, the methodological techniques of developmental psychology, such as the use of novel stimuli to control for past experience or control conditions to determine whether children are using simple associations, can be equally helpful for assessing the capacities of LLMs. In parallel, testing LLMs in this way can tell us whether the information that is encoded in text is sufficient to enable particular responses, or whether those responses depend on other kinds of information, such as information from exploration of the physical world. In this work we adapt classical developmental experiments to evaluate the capabilities of LaMDA, a large language model from Google. We propose a novel LLM Response Score (LRS) metric which can be used to evaluate other language models, such as GPT. We find that LaMDA generates appropriate responses that are similar to those of children in experiments involving social understanding, perhaps providing evidence that knowledge of these domains is discovered through language. On the other hand, LaMDA's responses in early object and action understanding, theory of mind, and especially causal reasoning tasks are very different from those of young children, perhaps showing that these domains require more real-world, self-initiated exploration and cannot simply be learned from patterns in language input.|
|**2023-05-18**|**Client Selection for Federated Policy Optimization with Environment Heterogeneity**|Zhijie Xie et.al.|[2305.10978v3](http://arxiv.org/abs/2305.10978v3)|null|The development of Policy Iteration (PI) has inspired many recent algorithms for Reinforcement Learning (RL), including several policy gradient methods, that gained both theoretical soundness and empirical success on a variety of tasks. The theory of PI is rich in the context of centralized learning, but its study is still in the infant stage under the federated setting. This paper explores the federated version of Approximate PI (API) and derives its error bound, taking into account the approximation error introduced by environment heterogeneity. We theoretically prove that a proper client selection scheme can reduce this error bound. Based on the theoretical result, we propose a client selection algorithm to alleviate the additional approximation error caused by environment heterogeneity. Experiment results show that the proposed algorithm outperforms other biased and unbiased client selection methods on the federated mountain car problem by effectively selecting clients with a lower level of heterogeneity from the population distribution.|
|**2023-05-17**|**Analyzing the Stance of Facebook Posts on Abortion Considering State-level Health and Social Compositions**|Ana Aleksandric et.al.|[2305.09889v1](http://arxiv.org/abs/2305.09889v1)|null|Abortion remains one of the most controversial topics, especially after overturning Roe v. Wade ruling in the United States. Previous literature showed that the illegality of abortion could have serious consequences, as women might seek unsafe pregnancy terminations leading to increased maternal mortality rates and negative effects on their reproductive health. Therefore, the stances of the abortion-related Facebook posts were analyzed at the state level in the United States from May 4 until June 30, 2022, right after the Supreme Court's decision was disclosed. In more detail, the pre-trained Transformer architecture-based model was fine-tuned on a manually labeled training set to obtain a stance detection model suitable for the collected dataset. Afterward, we employed appropriate statistical tests to examine the relationships between public opinion regarding abortion, abortion legality, political leaning, and factors measuring the overall population's health, health knowledge, and vulnerability per state. We found that states with a higher number of views against abortion also have higher infant and maternal mortality rates. Furthermore, the stance of social media posts per state is mostly matching with the current abortion laws in these states. While aligned with existing literature, these findings indicate how public opinion, laws, and women's and infants' health are related, and interventions are required to educate and protect women, especially in vulnerable populations.|
|**2023-05-16**|**Evaluation of self-supervised pre-training for automatic infant movement classification using wearable movement sensors**|Einari Vaaras et.al.|[2305.09366v1](http://arxiv.org/abs/2305.09366v1)|[link](https://github.com/SPEECHCOG/data2vec_maiju)|The recently-developed infant wearable MAIJU provides a means to automatically evaluate infants' motor performance in an objective and scalable manner in out-of-hospital settings. This information could be used for developmental research and to support clinical decision-making, such as detection of developmental problems and guiding of their therapeutic interventions. MAIJU-based analyses rely fully on the classification of infant's posture and movement; it is hence essential to study ways to increase the accuracy of such classifications, aiming to increase the reliability and robustness of the automated analysis. Here, we investigated how self-supervised pre-training improves performance of the classifiers used for analyzing MAIJU recordings, and we studied whether performance of the classifier models is affected by context-selective quality-screening of pre-training data to exclude periods of little infant movement or with missing sensors. Our experiments show that i) pre-training the classifier with unlabeled data leads to a robust accuracy increase of subsequent classification models, and ii) selecting context-relevant pre-training data leads to substantial further improvements in the classifier performance.|
|**2023-05-16**|**Health Impacts of Public Pawnshops in Industrializing Tokyo**|Tatsuki Inoue et.al.|[2305.09352v1](http://arxiv.org/abs/2305.09352v1)|null|This study is the first to investigate whether financial institutions for low-income populations have contributed to the historical decline in mortality rates. Using ward-level panel data from prewar Tokyo City, we found that public pawn loans were associated with reductions in infant and fetal death rates, potentially through improved nutrition and hygiene measures. Simple calculations suggest that popularizing public pawnshops led to a 6% and 8% decrease in infant mortality and fetal death rates, respectively, from 1927 to 1935. Contrarily, private pawnshops showed no significant association with health improvements. Our findings enrich the expanding literature on demographics and financial histories.|
|**2023-05-09**|**Child Palm-ID: Contactless Palmprint Recognition for Children**|Akash Godbole et.al.|[2305.05161v1](http://arxiv.org/abs/2305.05161v1)|null|Effective distribution of nutritional and healthcare aid for children, particularly infants and toddlers, in some of the least developed and most impoverished countries of the world, is a major problem due to the lack of reliable identification documents. Biometric authentication technology has been investigated to address child recognition in the absence of reliable ID documents. We present a mobile-based contactless palmprint recognition system, called Child Palm-ID, which meets the requirements of usability, hygiene, cost, and accuracy for child recognition. Using a contactless child palmprint database, Child-PalmDB1, consisting of 19,158 images from 1,020 unique palms (in the age range of 6 mos. to 48 mos.), we report a TAR=94.11% @ FAR=0.1%. The proposed Child Palm-ID system is also able to recognize adults, achieving a TAR=99.4% on the CASIA contactless palmprint database and a TAR=100% on the COEP contactless adult palmprint database, both @ FAR=0.1%. These accuracies are competitive with the SOTA provided by COTS systems. Despite these high accuracies, we show that the TAR for time-separated child-palmprints is only 78.1% @ FAR=0.1%.|
|**2023-05-03**|**Analysing the Impact of Audio Quality on the Use of Naturalistic Long-Form Recordings for Infant-Directed Speech Research**|María Andrea Cruz Blandón et.al.|[2305.01965v1](http://arxiv.org/abs/2305.01965v1)|[link](https://github.com/SPEECHCOG/ids_audio_quality_analysis)|Modelling of early language acquisition aims to understand how infants bootstrap their language skills. The modelling encompasses properties of the input data used for training the models, the cognitive hypotheses and their algorithmic implementations being tested, and the evaluation methodologies to compare models to human data. Recent developments have enabled the use of more naturalistic training data for computational models. This also motivates development of more naturalistic tests of model behaviour. A crucial step towards such an aim is to develop representative speech datasets consisting of speech heard by infants in their natural environments. However, a major drawback of such recordings is that they are typically noisy, and it is currently unclear how the sound quality could affect analyses and modelling experiments conducted on such data. In this paper, we explore this aspect for the case of infant-directed speech (IDS) and adult-directed speech (ADS) analysis. First, we manually and automatically annotated audio quality of utterances extracted from two corpora of child-centred long-form recordings (in English and French). We then compared acoustic features of IDS and ADS in an in-lab dataset and across different audio quality subsets of naturalistic data. Finally, we assessed how the audio quality and recording environment may change the conclusions of a modelling analysis using a recent self-supervised learning model. Our results show that the use of modest and high audio quality naturalistic speech data result in largely similar conclusions on IDS and ADS in terms of acoustic analyses and modelling experiments. We also found that an automatic sound quality assessment tool can be used to screen out useful parts of long-form recordings for a closer analysis with comparable results to that of manual quality annotation.|
|**2023-05-02**|**DeCom: Deep Coupled-Factorization Machine for Post COVID-19 Respiratory Syncytial Virus Prediction with Nonpharmaceutical Interventions Awareness**|Xinyan Li et.al.|[2305.01770v1](http://arxiv.org/abs/2305.01770v1)|null|Respiratory syncytial virus (RSV) is one of the most dangerous respiratory diseases for infants and young children. Due to the nonpharmaceutical intervention (NPI) imposed in the COVID-19 outbreak, the seasonal transmission pattern of RSV has been discontinued in 2020 and then shifted months ahead in 2021 in the northern hemisphere. It is critical to understand how COVID-19 impacts RSV and build predictive algorithms to forecast the timing and intensity of RSV reemergence in post-COVID-19 seasons. In this paper, we propose a deep coupled tensor factorization machine, dubbed as DeCom, for post COVID-19 RSV prediction. DeCom leverages tensor factorization and residual modeling. It enables us to learn the disrupted RSV transmission reliably under COVID-19 by taking both the regular seasonal RSV transmission pattern and the NPI into consideration. Experimental results on a real RSV dataset show that DeCom is more accurate than the state-of-the-art RSV prediction algorithms and achieves up to 46% lower root mean square error and 49% lower mean absolute error for country-level prediction compared to the baselines.|
|**2023-05-02**|**Self-supervised learning for infant cry analysis**|Arsenii Gorin et.al.|[2305.01578v1](http://arxiv.org/abs/2305.01578v1)|null|In this paper, we explore self-supervised learning (SSL) for analyzing a first-of-its-kind database of cry recordings containing clinical indications of more than a thousand newborns. Specifically, we target cry-based detection of neurological injury as well as identification of cry triggers such as pain, hunger, and discomfort. Annotating a large database in the medical setting is expensive and time-consuming, typically requiring the collaboration of several experts over years. Leveraging large amounts of unlabeled audio data to learn useful representations can lower the cost of building robust models and, ultimately, clinical solutions. In this work, we experiment with self-supervised pre-training of a convolutional neural network on large audio datasets. We show that pre-training with SSL contrastive loss (SimCLR) performs significantly better than supervised pre-training for both neuro injury and cry triggers. In addition, we demonstrate further performance gains through SSL-based domain adaptation using unlabeled infant cries. We also show that using such SSL-based pre-training for adaptation to cry sounds decreases the need for labeled data of the overall system.|
|**2023-05-01**|**CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds**|David Budaghyan et.al.|[2305.00969v3](http://arxiv.org/abs/2305.00969v3)|[link](https://github.com/ubenwa/cryceleb2023)|This paper describes the Ubenwa CryCeleb dataset - a labeled collection of infant cries, and the accompanying CryCeleb 2023 task - a public speaker verification challenge based on infant cry sounds. We release for academic usage more than 6 hours of manually segmented cry sounds from 786 newborns to encourage research in infant cry analysis.|
|**2023-04-27**|**Cluster Flow: how a hierarchical clustering layer make allows deep-NNs more resilient to hacking, more human-like and easily implements relational reasoning**|Ella Gale et.al.|[2304.14081v1](http://arxiv.org/abs/2304.14081v1)|null|Despite the huge recent breakthroughs in neural networks (NNs) for artificial intelligence (specifically deep convolutional networks) such NNs do not achieve human-level performance: they can be hacked by images that would fool no human and lack `common sense'. It has been argued that a basis of human-level intelligence is mankind's ability to perform relational reasoning: the comparison of different objects, measuring similarity, grasping of relations between objects and the converse, figuring out the odd one out in a set of objects. Mankind can even do this with objects they have never seen before. Here we show how ClusterFlow, a semi-supervised hierarchical clustering framework can operate on trained NNs utilising the rich multi-dimensional class and feature data found at the pre-SoftMax layer to build a hyperspacial map of classes/features and this adds more human-like functionality to modern deep convolutional neural networks. We demonstrate this with 3 tasks. 1. the statistical learning based `mistakes' made by infants when attending to images of cats and dogs. 2. improving both the resilience to hacking images and the accurate measure of certainty in deep-NNs. 3. Relational reasoning over sets of images, including those not known to the NN nor seen before. We also demonstrate that ClusterFlow can work on non-NN data and deal with missing data by testing it on a Chemistry dataset. This work suggests that modern deep NNs can be made more human-like without re-training of the NNs. As it is known that some methods used in deep and convolutional NNs are not biologically plausible or perhaps even the best approach: the ClusterFlow framework can sit on top of any NN and will be a useful tool to add as NNs are improved in this regard.|
|**2023-04-19**|**Detection of a Super-Virial Hot Component in the Milky Way Circumgalactic Medium Along Multiple Sight-Lines by Using the Stacking Technique**|Armando Lara-DI et.al.|[2304.09641v1](http://arxiv.org/abs/2304.09641v1)|null|The study of the elusive hot component ($T \gtrsim 10^7$ K) of the Milky Way circumgalactic medium (CGM) is a novel topic to understand Galactic formation and evolution. In this work, we use the stacking technique through 46 lines of sight with Chandra ACIS-S HETG totaling over 10Ms of exposure time and 9 lines of sight with ACIS-S LETG observations totaling over 1Ms of exposure time, to study in absorption the presence of highly ionized metals arising from the super-virial temperature phase of the CGM. Focusing in the spectral range $4 - 8$ $\r{A}$, we were able to confirm the presence of this hot phase with high significance. We detected transitions of Si XIV K$\alpha$ (with total significance of 6.0$\sigma$) and, for the first time, SXVI K (total significance 4.8$\sigma$) in the rest frame of our own Galaxy. For S XVI K$\alpha$ we found a column density of $1.50^{+0.44}_{-0.38} \times 10^{16} \mathrm{cm}^{-2}$. For Si XIV K$\alpha$ we measured a column density of $0.87\pm{0.16} \times 10^{16} \mathrm{cm}^{-2}$. The lines of sight used in this work are spread across the sky, probing widely separated regions of the CGM. Therefore, our results indicate that this newly discovered hot medium extends throughout the halo, and is not related only to the Galactic Bubbles. The hot gas location, distribution, and covering factor, however, remain unknown. This component might contribute significantly to the missing baryons and metals in the Milky Way.|
|**2023-04-08**|**Predator-prey dynamics pertaining to structuralizing predator species into three stages coupled with maturation delay owing to juvenile hunting**|Debasish Bhattacharjee et.al.|[2304.05159v1](http://arxiv.org/abs/2304.05159v1)|null|The predator-prey dynamic appertaining to two species is explored, wherein the predator species is structured into different stages. As evidenced from natural documentation, the immature predators possess the potential to predate albeit not as competently as the adults. Nevertheless, this potentiality is not acquired immediately after their incipience of life, hence, the immature stage is branched off into the infant stage, the stage with extensive reliance on the adults, and the juvenile stage, the stage with the potential to predate but not to procreate. In this paper, this inaugural concept is coupled with injuries in the juvenile stage as the repercussion of their incompetency in predating, thereby ensuing a delay in their maturation. With the incentive to investigate the ascendancy of these refinements over the whole system, stability analyses along with various bifurcation analyses around the equilibrium points of the system are corroborated. In addition to Hopf, transcritical, and saddle node bifurcations, the existence of Bogdanov-Takens point, cusp point, Bautin bifurcation point, bloom phenomenon, twice occurring Hopf bifurcation, and bi-stability phenomenon make the paper appreciably more rich and efficacious.|
|**2023-04-05**|**IHCV: Discovery of Hidden Time-Dependent Control Variables in Non-Linear Dynamical Systems**|Juan Munoz et.al.|[2304.02443v1](http://arxiv.org/abs/2304.02443v1)|null|Discovering non-linear dynamical models from data is at the core of science. Recent progress hinges upon sparse regression of observables using extensive libraries of candidate functions. However, it remains challenging to model hidden non-observable control variables governing switching between different dynamical regimes. Here we develop a data-efficient derivative-free method, IHCV, for the Identification of Hidden Control Variables. First, the performance and robustness of IHCV against noise are evaluated by benchmarking the IHCV method using well-known bifurcation models (saddle-node, transcritical, pitchfork, Hopf). Next, we demonstrate that IHCV discovers hidden driver variables in the Lorenz, van der Pol, Hodgkin-Huxley, and Fitzhugh-Nagumo models. Finally, IHCV generalizes to the case when only partial observational is given, as demonstrated using the toggle switch model, the genetic repressilator oscillator, and a Waddington landscape model. Our proof-of-principle illustrates that utilizing normal forms could facilitate the data-efficient and scalable discovery of hidden variables controlling transitions between different dynamical regimes and non-linear models.|
|**2023-04-02**|**Origin of high-velocity ejecta and early red excess emission in the infant Type Ia supernova 2021aefx**|Yuan Qi Ni et.al.|[2304.00625v2](http://arxiv.org/abs/2304.00625v2)|null|\object{SN 2021aefx} is a normal Type Ia Supernova (SN) with red excess emission over the first $\sim$ 2 days. We present detailed analysis of this SN using our high-cadence KMTNet multi-band photometry, spectroscopy, and publicly available data. We provide the first measurements of its epochs of explosion (MJD 59529.32 $\pm$ 0.16) as well as ``first light'' (MJD 59529.85 $\pm$ 0.55) associated with the main ejecta ${\rm{^{56}Ni}}$ distribution. This places our first detection of SN 2021aefx at $\sim -$0.5 hours since ``first light'', indicating the presence of additional power sources. Our peak-spectrum confirms its Type Ia sub-classification as intermediate between Core-Normal and Broad-Line, and we estimate the ejecta mass to be $\sim$ 1.34 $M_{\odot}$. The pre-peak spectral evolution identifies fast-expanding material reaching $>$ 40,000 km s$^{-1}$ (the fastest ever observed in Type Ia SNe) and at least two distinct homologously-expanding ejecta components: (1) a normal-velocity (12,400 km s$^{-1}$) component consistent with the typical photospheric evolution of Chandrasekhar-mass ejecta; and (2) a high-velocity (23,500 km s$^{-1}$) component visible during the first $\sim$ 3.6 days post-explosion, which locates the component within the outer $<$ 16\% of the ejecta mass. Asymmetric, subsonic explosion processes producing a non-spherical photosphere provide an explanation for the simultaneous presence of the two components, as well as the red excess emission via a slight ${\rm{^{56}Ni}}$ enrichment in the outer $\sim$ 0.5\% of the ejecta mass. Our spectrum from 300 days post-peak advances the constraint against non-degenerate companions and further supports a near-Chandrasekhar-mass explosion origin. Off-center ignited delayed-detonations of Chandrasekhar-mass white dwarfs may be responsible for the observed features of SN 2021aefx in some normal Type Ia SNe.|
|**2023-04-02**|**The short- and long-term determinants of fertility in Uruguay**|Zuleika Ferre et.al.|[2304.00539v1](http://arxiv.org/abs/2304.00539v1)|null|This paper examines the determinants of fertility among women at different stages of their reproductive lives in Uruguay. To this end, we employ time series analysis methods based on data from 1968 to 2021 and panel data techniques based on department-level statistical information from 1984 to 2019. The results of our first econometric exercise indicate a cointegration relationship between fertility and economic performance, education and infant mortality, with differences observed by reproductive stage. We find a negative relationship between income and fertility for women aged 20-29 that persists for women aged 30 and over. This result suggests that having children is perceived as an opportunity cost for women in this age group. We also observe a negative relationship between education and adolescent fertility, which has implications for the design of public policies. A panel data analysis with econometric techniques allowing us to control for unobserved heterogeneity confirms that income is a relevant factor for all groups of women and reinforces the crucial role of education in reducing teenage fertility. We also identify a negative correlation between fertility and employment rates for women aged 30 and above. We outline some possible explanations for these findings in the context of work-life balance issues and argue for the importance of implementing social policies to address them.|
|**2023-04-01**|**N,N,N-Trimethyl chitosan as a permeation enhancer for inhalation drug delivery: interaction with a model pulmonary surfactant**|Jana Szabová et.al.|[2304.04547v1](http://arxiv.org/abs/2304.04547v1)|null|N,N,N-Trimethyl chitosan (TMC), a biocompatible and biodegradable derivative of chitosan, is currently used as a permeation enhancer to increase the translocation of drugs to the bloodstream in the lungs. This article discusses the effect of TMC on a mimetic pulmonary surfactant, Curosurf, a low-viscosity lipid formulation administered to preterm infants with acute respiratory distress syndrome. Curosurf exhibits a strong interaction with TMC, resulting in the formation of aggregates at electrostatic charge stoichiometry. At nanoscale, Curosurf undergoes a profound reorganization of its lipid vesicles in terms of size and lamellarity. The initial micron-sized vesicles (average size 4.8 microns) give way to a froth-like network of unilamellar vesicles about 300 nm in size. Under such conditions, neutralization of the cationic charges by pulmonary surfactant may inhibit TMC permeation enhancer capacity, especially as electrostatic charge complexation is found at low TMC content. The permeation properties of pulmonary surfactant-neutralized TMC should then be evaluated for its applicability as a permeation enhancer for inhalation in the alveolar region.|
|**2023-03-31**|**Symmetry Groupoids for Pattern-Selective Feedback Stabilization of the Chafee--Infante Equation**|Isabelle Schneider et.al.|[2303.18078v2](http://arxiv.org/abs/2303.18078v2)|null|Reaction-diffusion equations are ubiquitous in various scientific domains and their patterns represent a fascinating area of investigation. However, many of these patterns are unstable and therefore challenging to observe. To overcome this limitation, we present new noninvasive feedback controls based on symmetry groupoids. As a concrete example, we employ these controls to selectively stabilize unstable equilibria of the Chafee--Infante equation under Dirichlet boundary conditions on the interval. Unlike conventional reflection-based control schemes, our approach incorporates additional symmetries that enable us to design new convolution controls for stabilization. By demonstrating the efficacy of our method, we provide a new tool for investigating and controlling systems with unstable patterns, with potential implications for a wide range of scientific disciplines.|
|**2023-03-29**|**A Video-based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants**|Shaotong Zhu et.al.|[2303.16867v1](http://arxiv.org/abs/2303.16867v1)|[link](https://github.com/ostadabbas/nns-detection-and-segmentation)|We present an end-to-end computer vision pipeline to detect non-nutritive sucking (NNS) -- an infant sucking pattern with no nutrition delivered -- as a potential biomarker for developmental delays, using off-the-shelf baby monitor video footage. One barrier to clinical (or algorithmic) assessment of NNS stems from its sparsity, requiring experts to wade through hours of footage to find minutes of relevant activity. Our NNS activity segmentation algorithm solves this problem by identifying periods of NNS with high certainty -- up to 94.0\% average precision and 84.9\% average recall across 30 heterogeneous 60 s clips, drawn from our manually annotated NNS clinical in-crib dataset of 183 hours of overnight baby monitor footage from 19 infants. Our method is based on an underlying NNS action recognition algorithm, which uses spatiotemporal deep learning networks and infant-specific pose estimation, achieving 94.9\% accuracy in binary classification of 960 2.5 s balanced NNS vs. non-NNS clips. Tested on our second, independent, and public NNS in-the-wild dataset, NNS recognition classification reaches 92.3\% accuracy, and NNS segmentation achieves 90.8\% precision and 84.2\% recall.|
